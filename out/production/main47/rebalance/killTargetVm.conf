hydra.Prms-testRequirement = "Killing the rebalancing VM or a source or destination vm (for movingBuckets or movingPrimaries) does not impair the system.  Note that dataLoss is expected (as we are killing dataStore VMs), so careful validation cannot be done."; 

hydra.Prms-testDescription = "Based on concParRegStopStart with an additional rebalancing VM.  The dataStores execute random PR operations while the rebalancer executes concurrently.  The test configures a ResourceObserver targetEvent which targets the rebalancer (rebalancingStarted), or the source or destination vm involved in movingBucket or movingPrimary).";

INCLUDE $JTESTS/hydraconfig/hydraparams1.inc;
INCLUDE $JTESTS/hydraconfig/topology_p2p_2.inc;

INITTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_initialize
             hydra.ConfigPrms-cacheConfig = cache1
             hydra.ConfigPrms-regionConfig = clientRegion
             threadGroups = peer
             runMode = always
             ;

// need some data in the region prior to the rebalancer initializing
// so we'll have a need to move primaries during rebalance
INITTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_doEntryOpsDataLoss
             threadGroups = peer
             ;

// The rebalancer will need an initialized cache
// initialize now (recoveryDelays are set to NEVER) so that there
// will be work to do as we add capacity
INITTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_initialize
             hydra.ConfigPrms-cacheConfig = cache1
             hydra.ConfigPrms-regionConfig = clientRegion
             threadGroups = rebalance
             runMode = always
             ;

// for targetEvent.rebalancingStarted, validation is done on rebalance restart
INITTASK     taskClass   = rebalance.RebalanceTest taskMethod = HydraTask_checkPartitionedRegionDetails
             threadGroups = rebalance
             runMode = dynamic
             ;

TASK         taskClass   = parReg.ParRegTest  taskMethod = HydraTask_doEntryOpsDataLoss
             threadGroups = peer
             ;

TASK         taskClass   = rebalance.RebalanceTest taskMethod = HydraTask_rebalanceTask
             threadGroups = rebalance
             ;

CLOSETASK    taskClass   = rebalance.RebalanceTest taskMethod = HydraTask_rebalanceTask
             rebalance.RebalancePrms-verifyBalance = true
             rebalance.RebalancePrms-rebalanceAction = none
             threadGroups = rebalance
             ;

CLOSETASK    taskClass   = parReg.ParRegTest  taskMethod = HydraTask_logLocalSize
             threadGroups = peer
             ;

ENDTASK      taskClass = rebalance.RebalanceTest   taskMethod = HydraTask_verifyTargetEventsProcessed
             clientNames  = fcn "hydra.TestConfigFcns.generateNames
                                 (\"${B}\", ${${B}Hosts}, true)"
                            ncf;

hydra.GemFirePrms-stopSystemsAfterTest = true;

hydra.Prms-totalTaskTimeSec = 600;  
hydra.Prms-maxResultWaitSec = 600;
hydra.Prms-serialExecution = false;
hydra.Prms-clientShutdownHook = parReg.ParRegUtil dumpAllPartitionedRegions;

INCLUDE $JTESTS/util/randomValues.inc;
util.RandomValuesPrms-objectType = byte[];
util.RandomValuesPrms-elementSize = 5000;
util.ValueHolderPrms-useExtraObject = true;
hydra.GemFirePrms-conserveSockets = ONEOF true false FOENO;

hydra.CachePrms-names           = cache1;
hydra.CachePrms-searchTimeout   = 600;

hydra.RegionPrms-names          = clientRegion;
hydra.RegionPrms-regionName     = partitionedRegion;
hydra.RegionPrms-cacheLoader    = parReg.ParRegLoader;
hydra.RegionPrms-dataPolicy     = partition;
hydra.RegionPrms-partitionName     = peerPR;

hydra.PartitionPrms-names          = peerPR;
hydra.PartitionPrms-localMaxMemory = default;
hydra.PartitionPrms-redundantCopies = 1;

// don't recover redundancy automatically, rely on rebalance
hydra.PartitionPrms-recoveryDelay = -1;
hydra.PartitionPrms-startupRecoveryDelay = -1;

util.TestHelperPrms-minTaskGranularitySec = 45;

// lynn - local destroy not supported for partitioned regions for Congo
// lynn - local invalidate not supported for partitioned regions for Congo
// The following 3 parameters specify random operations.
// The test will try to hold the size of the region to regionSizeThreshold.
// If the region grows larger in size than regionSizeThreshold, then the
// operations will be chosen from thresholdOperations, otherwise the operations
// will be chosen from entryOperations
parReg.ParRegPrms-entryOperations = ONEOF add add getNew getNew update invalidate get destroy FOENO; 
parReg.ParRegPrms-upperThreshold = 20000;
parReg.ParRegPrms-upperThresholdOperations = ONEOF destroy FOENO;
parReg.ParRegPrms-lowerThreshold = 10000;
parReg.ParRegPrms-lowerThresholdOperations = ONEOF add getNew FOENO;

util.CachePrms-useDeclarativeXmlFile = ONEOF true false FOENO;

THREADGROUP peer
    totalThreads = fcn
                   ${${A}Hosts} * ${${A}VMsPerHost} * ${${A}ThreadsPerVM}
                   ncf
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"${A}\", ${${A}Hosts}, true)"
                   ncf;

THREADGROUP rebalance
   totalThreads = fcn ${${B}Hosts} * ${${B}VMsPerHost} ncf
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"${B}\", ${${B}Hosts}, true)"
                   ncf;

//THREADGROUP killer
//    totalThreads = fcn ${${B}Hosts} * ${${B}VMsPerHost} ncf
//    clientNames  = fcn "hydra.TestConfigFcns.generateNames
//                        (\"${B}\", ${${B}Hosts}, true)"
//                   ncf;

rebalance.RebalancePrms-resourceObserver = rebalance.HAResourceObserver;
rebalance.RebalancePrms-targetEvent = ${targetEvent};
rebalance.RebalancePrms-actionDelaySecs = ${actionDelay};
rebalance.RebalancePrms-rebalanceAction = kill;
