include $JTESTS/hydraconfig/hydraparams1.inc;
include $JTESTS/hydraconfig/performance.inc;
include $JTESTS/hydraconfig/topology_5_locator.inc;

hydra.Prms-testDescription = "The goal of this test is to look at MTBF, running for ${totalTaskTimeSec} seconds, for a relatively large-scaled system involving datahosts, servers, feeds, and clients, along with with multiple locators and agents.  The datahosts preload DataSerializable accounts with 20-30 assets each, for an average of about 600 bytes per account.  The data is loaded into four different regions: replicate, persistentReplicate, partitioned, and partitionedReplicate.  The replicated regions are small in comparison to the partitioned regions.  Each datahost and server hosts half of the regions, one partitioned and one replicated, with one of those being persistent.  Clients and peer feeds carry out random cache operations on all regions.  Feeds do puts only.  Clients do over 50% gets, less than 50% puts, with the occasional getAll or putAll, using their own randomly chosen keys.  A small subset of the clients are empty and subscribe to all keys.  The rest have a local cache with heap eviction and subscribe to a small uniformly distributed random subset of the keys in each region.  Every ${bounceFrequency} seconds, the test takes down a server or datahost with a randomly selected nice or mean kill, waits from 1-4 minutes, then restarts it.  Client Java threads are cycled at hydra task boundaries every ${batchRangeMinSec} to ${batchRangeMaxSec} seconds.  Throughput is throttled, and response times and end-to-end latencies are recorded.  The test does no functional validation.";

hydra.Prms-testTopology = "The test configures ${dataHosts} datahosts and ${serverHosts} empty servers.  There are ${feedHosts} empty peer feeds with ${feedThreadsPerVM} threads each and ${clientHosts} clients with ${clientThreadsPerVM} threads each.  There are also ${locatorHosts} locators and ${agentHosts} agents.

hydra.Prms-testRequirement = "The test runs without failure for 24 hours.  Latencies are reasonable and do not spike.  The load is balanced for clients per server, queues per server, buckets per datahost, cpu per server/datahost, etc.  There are no leaks in memory, file descriptors, etc.  Server failover and partitioned region rebalancing perform well and cause no major dips in client or feed throughput.  The heap is well managed and does not experience long GC pauses or expensive collections, even though garbage is routinely collected from tenured heap.";

cacheperf.poc.useCase3_2.UseCase3Prms-taskTerminatorMethod = terminateOnTotalTaskTimeSec;
hydra.Prms-totalTaskTimeSec = ${totalTaskTimeSec};

cacheperf.poc.useCase3_2.UseCase3Prms-batchTerminatorMethod = terminateOnBatchSeconds;
cacheperf.poc.useCase3_2.UseCase3Prms-batchSeconds = ${batchSec};

hydra.Prms-maxResultWaitSec = fcn ${batchRangeMaxSec} + 600 ncf;

//------------------------------------------------------------------------------
// Threadgroups
//------------------------------------------------------------------------------

THREADGROUP locator_bouncer totalThreads = 1 clientNames  = locator1;
THREADGROUP locator
  totalThreads = fcn
                 ${locatorHosts} * ${locatorVMsPerHost} * ${locatorThreadsPerVM} - 1
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"locator\", ${locatorHosts}, true)"
                 ncf;
THREADGROUP feed
  totalThreads = fcn
                 ${feedHosts} * ${feedVMsPerHost} * ${feedThreadsPerVM}
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"feed\", ${feedHosts}, true)"
                 ncf;
THREADGROUP dataA
  totalThreads = fcn
                 ${dataHosts} * ${dataVMsPerHost} * ${dataThreadsPerVM} / 2
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"data\", ${dataHosts}/2, true)"
                 ncf;
THREADGROUP dataB
  totalThreads = fcn
                 ${dataHosts} * ${dataVMsPerHost} * ${dataThreadsPerVM} / 2
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"data\", ${dataHosts}/2, ${dataHosts}/2 + 1, true)"
                 ncf;
THREADGROUP serverA
  totalThreads = fcn
                 ${serverHosts} * ${serverVMsPerHost} * ${serverThreadsPerVM} / 2
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"server\", ${serverHosts}/2, true)"
                 ncf;
THREADGROUP serverB
  totalThreads = fcn
                 ${serverHosts} * ${serverVMsPerHost} * ${serverThreadsPerVM} / 2
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"server\", ${serverHosts}/2, ${serverHosts}/2 + 1, true)"
                 ncf;
THREADGROUP clientA
  totalThreads = fcn
                 ${clientHosts} * ${clientVMsPerHost} * ${clientThreadsPerVM} / 10
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"client\", ${clientHosts}/10, true)"
                 ncf;
THREADGROUP clientB
  totalThreads = fcn
                 ${clientHosts} * ${clientVMsPerHost} * ${clientThreadsPerVM} * 9 / 10
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"client\", ${clientHosts} * 9/10, ${clientHosts}/10 + 1, true)"
                 ncf;
THREADGROUP agent
  totalThreads = fcn
                 ${agentHosts} * ${agentVMsPerHost} * ${agentThreadsPerVM}
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"agent\", ${agentHosts}, true)"
                 ncf;

//------------------------------------------------------------------------------
// Tasks...
//
//------------------------------------------------------------------------------
// ...locators and agents...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createAndStartLocatorTask    
          threadGroups = locator_bouncer, locator
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createAndStartAgentTask    
          threadGroups = agent
          hydra.ConfigPrms-agentConfig = agent
          ;

//------------------------------------------------------------------------------
// ...statistics...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = openStatisticsTask
          threadGroups = locator_bouncer, dataA, dataB, serverA, serverB
          runMode = always
          ;

//------------------------------------------------------------------------------
// ...caches...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createCacheTask
          hydra.ConfigPrms-cacheConfig = cache
          runMode = always
          threadGroups = dataA, dataB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createCacheTask
          hydra.ConfigPrms-cacheConfig = cache
          runMode = always
          threadGroups = serverA, serverB
          ;

//------------------------------------------------------------------------------
// ...regions...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createRegionsTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionConfigs =
                    dataPar dataRepPersist
          runMode = always
          threadGroups = dataA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createRegionsTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionConfigs =
                    dataParPersist dataRep
          runMode = always
          threadGroups = dataB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createRegionsTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionConfigs =
                    serverPar serverRepPersist
          runMode = always
          threadGroups = serverA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createRegionsTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionConfigs =
                    serverParPersist serverRep
          runMode = always
          threadGroups = serverB
          ;

//------------------------------------------------------------------------------
// ...servers...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = startServerTask
          hydra.ConfigPrms-bridgeConfig = serverA
          runMode = always
          threadGroups = serverA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = startServerTask
          hydra.ConfigPrms-bridgeConfig = serverB
          runMode = always
          threadGroups = serverB
          ;

//------------------------------------------------------------------------------
// ...data creation...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createDataTask
          batch
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = Par
          cacheperf.poc.useCase3_2.UseCase3Prms-keyAllocation = ownKeysChunked
          cacheperf.poc.useCase3_2.UseCase3Prms-taskTerminatorMethod = terminateOnMaxKey
          threadGroups = dataA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createDataTask
          batch
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = ParPersist
          cacheperf.poc.useCase3_2.UseCase3Prms-keyAllocation = ownKeysChunked
          cacheperf.poc.useCase3_2.UseCase3Prms-taskTerminatorMethod = terminateOnMaxKey
          threadGroups = dataB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createDataTask
          batch
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = RepPersist
          cacheperf.poc.useCase3_2.UseCase3Prms-keyAllocation = ownKeysChunked
          cacheperf.poc.useCase3_2.UseCase3Prms-taskTerminatorMethod = terminateOnMaxKey
          threadGroups = dataA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createDataTask
          batch
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = Rep
          cacheperf.poc.useCase3_2.UseCase3Prms-keyAllocation = ownKeysChunked
          cacheperf.poc.useCase3_2.UseCase3Prms-taskTerminatorMethod = terminateOnMaxKey
          threadGroups = dataB
          ;

//------------------------------------------------------------------------------
// ...feeds...connect, cache, regions...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = openStatisticsTask
          threadGroups = feed
          runMode = always
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createCacheTask
          hydra.ConfigPrms-cacheConfig = cache
          threadGroups = feed
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createRegionsTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionConfigs =
                    feedPar feedParPersist
                    feedRepPersist feedRep
          threadGroups = feed
          ;

//------------------------------------------------------------------------------
// ...clients...connect, cache, regions/pools, interest registration...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = openStatisticsTask
          threadGroups = clientA, clientB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createCacheTask
          hydra.ConfigPrms-cacheConfig = cache
          threadGroups = clientA, clientB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createRegionsTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionConfigs =
                    emptyClientPar emptyClientParPersist
                    emptyClientRepPersist emptyClientRep
          threadGroups = clientB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = createRegionsTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionConfigs =
                    clientPar clientParPersist
                    clientRepPersist clientRep
          threadGroups = clientA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = registerInterestRegexAllTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = Par
          threadGroups = clientB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = registerInterestRegexAllTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = ParPersist
          threadGroups = clientB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = registerInterestRegexAllTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = RepPersist
          threadGroups = clientB
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = registerInterestRegexAllTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = Rep
          threadGroups = clientB
          ;

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = registerInterestTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = Par
          cacheperf.poc.useCase3_2.UseCase3Prms-interestTotalKeys =
            fcn ${parKeysPerDatahost} * ${dataHosts} * ${dataVMsPerHost}/10 ncf
          cacheperf.poc.useCase3_2.UseCase3Prms-interestBatchSize = 1000
          threadGroups = clientA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = registerInterestTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = ParPersist
          cacheperf.poc.useCase3_2.UseCase3Prms-interestTotalKeys =
            fcn ${parPersistKeysPerDatahost} * ${dataHosts} * ${dataVMsPerHost}/10 ncf
          cacheperf.poc.useCase3_2.UseCase3Prms-interestBatchSize = 1000
          threadGroups = clientA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = registerInterestTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = RepPersist
          cacheperf.poc.useCase3_2.UseCase3Prms-interestTotalKeys =
            fcn ${repPersistKeys}/10 ncf
          cacheperf.poc.useCase3_2.UseCase3Prms-interestBatchSize = 1000
          threadGroups = clientA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = registerInterestTask
          cacheperf.poc.useCase3_2.UseCase3Prms-regionName = Rep
          cacheperf.poc.useCase3_2.UseCase3Prms-interestTotalKeys =
            fcn ${repKeys}/10 ncf
          cacheperf.poc.useCase3_2.UseCase3Prms-interestBatchSize = 1000
          threadGroups = clientA
          ;

//------------------------------------------------------------------------------
// ...rebalance...
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = rebalanceTask
          runMode = dynamic
          threadGroups = dataA
          ;
INITTASK  taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = rebalanceTask
          runMode = dynamic
          threadGroups = dataB
          ;

//------------------------------------------------------------------------------
// ...feeds and clients...
//------------------------------------------------------------------------------

TASK taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = putDataTask
     cacheperf.poc.useCase3_2.UseCase3Prms-keyAllocation = ownKeysRandomWrap
     cacheperf.poc.useCase3_2.UseCase3Prms-batchSeconds =
          range ${batchRangeMinSec} ${batchRangeMaxSec} egnar
     threadGroups = feed
     ;
TASK taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = mixDataTask
     cacheperf.poc.useCase3_2.UseCase3Prms-keyAllocation = ownKeysRandomWrap
     cacheperf.poc.useCase3_2.UseCase3Prms-batchSeconds =
          range ${batchRangeMinSec} ${batchRangeMaxSec} egnar
     threadGroups = clientA, clientB
     ;

//------------------------------------------------------------------------------
// ...bounces...
//------------------------------------------------------------------------------

TASK taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = bounceTask
     maxThreads = 1 startInterval = 60 endInterval = ${bounceFrequency}
     threadGroups = locator_bouncer
     ;

cacheperf.poc.useCase3_2.UseCase3Prms-clientNameToBounce = robing server data gnibor;
cacheperf.poc.useCase3_2.UseCase3Prms-useMeanKill = oneof true false foeno;
//cacheperf.poc.useCase3_2.UseCase3Prms-stopWaitSec = range 300 400 egnar;
cacheperf.poc.useCase3_2.UseCase3Prms-restartWaitSec = oneof 60 120 180 240 foeno;

//------------------------------------------------------------------------------
// ...shutdown...
//------------------------------------------------------------------------------

CLOSETASK taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = closeCacheTask
          threadGroups = clientA, clientB
          ;
CLOSETASK taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = closeCacheTask
          runMode = always
          threadGroups = serverA, serverB
          ;
CLOSETASK taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = closeCacheTask
          runMode = always
          threadGroups = dataA, dataB
          ;
CLOSETASK taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = closeCacheTask
          threadGroups = feed
          ;
CLOSETASK taskClass = cacheperf.poc.useCase3_2.UseCase3Client taskMethod = closeStatisticsTask
          runMode = always
          threadGroups = locator_bouncer, feed, dataA, dataB, serverA, serverB, clientA, clientB
          ;

//------------------------------------------------------------------------------
// Distributed system
//------------------------------------------------------------------------------

hydra.GemFirePrms-distributedSystem =
  fcn "hydra.TestConfigFcns.duplicate(\"ds\",    ${locatorHosts})" ncf
  fcn "hydra.TestConfigFcns.duplicate(\"ds\",    ${feedHosts})" ncf
  fcn "hydra.TestConfigFcns.duplicate(\"ds\",    ${dataHosts})" ncf
  fcn "hydra.TestConfigFcns.duplicate(\"ds\",    ${serverHosts})" ncf
  fcn "hydra.TestConfigFcns.duplicate(\"loner\", ${clientHosts})" ncf
  fcn "hydra.TestConfigFcns.duplicate(\"ds\",    ${agentHosts})" ncf
  ;
hydra.GemFirePrms-deltaPropagation = false;

//------------------------------------------------------------------------------
// Agent
//------------------------------------------------------------------------------

hydra.AgentPrms-names = agent;
hydra.AgentPrms-adminName = admin;

hydra.AdminPrms-names = admin;
hydra.AdminPrms-distributedSystem = ds;

//------------------------------------------------------------------------------
// Cache
//------------------------------------------------------------------------------

hydra.CachePrms-names = cache;

//------------------------------------------------------------------------------
// Regions
//------------------------------------------------------------------------------

cacheperf.poc.useCase3_2.UseCase3Prms-regionSpec =
  Par
  fcn ${parKeysPerDatahost} * ${dataHosts} * ${dataVMsPerHost} ncf
  ,
  ParPersist
  fcn ${parPersistKeysPerDatahost} * ${dataHosts} * ${dataVMsPerHost} ncf
  ,
  RepPersist
  ${repPersistKeys}
  ,
  Rep
  ${repKeys}
  ;

cacheperf.poc.useCase3_2.UseCase3Prms-regionName =
          oneof Par ParPersist Rep RepPersist foeno;

hydra.RegionPrms-names =
  feedPar        feedParPersist        feedRepPersist        feedRep
  dataPar        dataParPersist        dataRepPersist        dataRep
  serverPar      serverParPersist      serverRepPersist      serverRep
  emptyClientPar emptyClientParPersist emptyClientRepPersist emptyClientRep
  clientPar      clientParPersist      clientRepPersist      clientRep
  ;
hydra.RegionPrms-regionName =
  Par            ParPersist            RepPersist            Rep
  Par            ParPersist            RepPersist            Rep
  Par            ParPersist            RepPersist            Rep
  Par            ParPersist            RepPersist            Rep
  Par            ParPersist            RepPersist            Rep
  ;
hydra.RegionPrms-cacheListeners =
  none,          none,                 none,                 none,
  none,          none,                 none,                 none,
  none,          none,                 none,                 none,
  cacheperf.poc.useCase3_2.LatencyListener
  ;
hydra.RegionPrms-dataPolicy =
  partition      partition             replicate             replicate
  partition      persistentPartition   persistentReplicate   replicate
  partition      partition             empty                 empty
  empty          empty                 empty                 empty
  default        default               default               default
  ;
hydra.RegionPrms-diskStoreName =
  none           none                  none                  none
  none           disk                  disk                  none
  none           none                  none                  none
  none           none                  none                  none
  none           none                  none                  none
  ;
hydra.RegionPrms-diskSynchronous = true
  ;
hydra.RegionPrms-enableSubscriptionConflation = false
  ;
hydra.RegionPrms-evictionAttributes =
  none,          none,                 none,                 none,
  none,          none,                 none,                 none,
  none,          none,                 none,                 none,
  none,          none,                 none,                 none,
  lruHeapPercentage none localDestroy
  ;
hydra.RegionPrms-interestPolicy =
  default        default               default               default
  default        default               default               default
  default        default               default               default
  all            all                   all                   all
  all            all                   all                   all
  ;
hydra.RegionPrms-partitionName =
  accessor       accessor              none                  none
  datahost       datahost              none                  none
  accessor       accessor              none                  none
  none           none                  none                  none
  none           none                  none                  none
  ;
hydra.RegionPrms-poolName =
  none           none                  none                  none
  none           none                  none                  none
  none           none                  none                  none
  poolA          poolB                 poolA                 poolB
  poolA          poolB                 poolA                 poolB
  ;
hydra.RegionPrms-scope =
  default        default               dack                  dack
  default        default               dack                  dack
  default        default               dack                  dack
  local          local                 local                 local
  local          local                 local                 local
  ;

hydra.ResourceManagerPrms-evictionHeapPercentage = 70;

hydra.PartitionPrms-names           = accessor datahost;
hydra.PartitionPrms-localMaxMemory  = 0        default;
hydra.PartitionPrms-redundantCopies = 1;
hydra.PartitionPrms-totalNumBuckets =
      fcn ${bucketsPerDatahost} * ${dataHosts} * ${dataVMsPerHost} ncf;

hydra.DiskStorePrms-names = disk;
hydra.DiskStorePrms-queueSize = 1000;
hydra.DiskStorePrms-diskDirNum = 1;
hydra.DiskStorePrms-diskDirSizes = 30000;

hydra.Prms-removeDiskFilesAfterTest = true;

//------------------------------------------------------------------------------
// Bridge Server
//------------------------------------------------------------------------------

hydra.BridgePrms-names  = serverA  serverB;
hydra.BridgePrms-groups = serverA, serverB;
hydra.BridgePrms-maximumMessageCount = 1000;

//------------------------------------------------------------------------------
// Edge Client Pool
//------------------------------------------------------------------------------

hydra.PoolPrms-names                  = poolA   poolB;
hydra.PoolPrms-freeConnectionTimeout  = 10000000;
hydra.PoolPrms-idleTimeout            = 120000;
hydra.PoolPrms-prSingleHopEnabled     = false;
hydra.PoolPrms-readTimeout            = 10000000;
hydra.PoolPrms-serverGroup            = serverA serverB;
hydra.PoolPrms-subscriptionEnabled    = true;
hydra.PoolPrms-subscriptionRedundancy = 1;
hydra.PoolPrms-threadLocalConnections = false;

//------------------------------------------------------------------------------
// Data
//------------------------------------------------------------------------------

cacheperf.poc.useCase3_2.UseCase3Prms-keyType = java.lang.Long;
cacheperf.poc.useCase3_2.UseCase3Prms-objectType = objects.FastAssetAccount;
objects.FastAssetAccountPrms-size = range 20 30 egnar;
objects.FastAssetAccountPrms-encodeTimestamp = true;

cacheperf.poc.useCase3_2.UseCase3Prms-allowNulls = false;
cacheperf.poc.useCase3_2.UseCase3Prms-bulkOpMapSize = range 50 75 egnar;
cacheperf.poc.useCase3_2.UseCase3Prms-cacheOpSpec =
          get    75.000,
          getAll  0.010,
          putAll  0.001,
          put    24.989
          ;
cacheperf.poc.useCase3_2.UseCase3Prms-sleepAfterOpMs = range 0 15000 egnar;

//------------------------------------------------------------------------------
// Versioning (allows running current tests against old builds)
//------------------------------------------------------------------------------

hydra.ClientPrms-versionNames = version;

hydra.VersionPrms-names   = version;
hydra.VersionPrms-version = ${version};

//------------------------------------------------------------------------------
// Performance
//------------------------------------------------------------------------------

hydra.GemFirePrms-conserveSockets = false;

hydra.VmPrms-extraClassPaths =
  fcn "hydra.TestConfigFcns.duplicateString(\"none\", ${locatorHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"none\", ${feedHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"none\", ${dataHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"none\", ${serverHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"none\", ${clientHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"$GEMFIRE/lib/commons-logging-1.1.1.jar:$GEMFIRE/lib/commons-modeler-2.0.jar:$GEMFIRE/lib/mx4j.jar:$GEMFIRE/lib/mx4j-remote.jar:$GEMFIRE/lib/mx4j-tools.jar:$GEMFIRE/lib/mail.jar\", ${agentHosts}, true)" ncf
  ;

hydra.VmPrms-extraVMArgs +=
  fcn "hydra.TestConfigFcns.duplicateString(\"-Dnone\", ${locatorHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms256m -Xmx256m\", ${feedHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms${dataHeapMB}m -Xmx${dataHeapMB}m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=33 -XX:+DisableExplicitGC\", ${dataHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms${serverHeapMB}m -Xmx${serverHeapMB}m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=33 -XX:+DisableExplicitGC\", ${serverHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms${clientHeapMB}m -Xmx${clientHeapMB}m\", ${clientHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Dnone\", ${agentHosts}, true)" ncf
  ;

hydra.timeserver.TimeServerPrms-clockSkewUpdateFrequencyMs = 1000;
hydra.timeserver.TimeServerPrms-clockSkewMaxLatencyMs = 25;

perffmwk.PerfReportPrms-generatePerformanceReport = false;
