hydra.Prms-testDescription = "Peer datahosts to load, add new datahost, rebalance and recover from disk files with no values.";

//------------------------------------------------------------------------------
// Force a gc in all vms. (This occurs here before the include so it will be
// the first close task executed)
//------------------------------------------------------------------------------

CLOSETASK taskClass = cacheperf.CachePerfClient taskMethod = doGC
          threadGroups = peer, extra;

include $JTESTS/cacheperf/comparisons/parReg/persist/peerDataCreate.conf;
include $JTESTS/hydraconfig/topology_3_locator.inc;

THREADGROUP extra
    totalThreads = fcn
                   ${extraHosts} * ${extraVMsPerHost} * ${extraThreadsPerVM}
                   ncf
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"extra\", ${extraHosts}, true)"
                   ncf;

THREADGROUP controller
    totalThreads = fcn
                   ${controllerHosts} * ${controllerVMsPerHost} * ${controllerThreadsPerVM}
                   ncf
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"controller\", ${controllerHosts}, true)"
                   ncf;

hydra.RegionPrms-evictionAttributes = lruEntryCount 3463683 overflowToDisk;

//------------------------------------------------------------------------------
// Create adminVM
//------------------------------------------------------------------------------
INITTASK     taskClass     = util.AdminHelper  taskMethod = HydraTask_initializeAdminDS
             threadGroups = controller;

//------------------------------------------------------------------------------
// Task to add a new vm and rebalance
//------------------------------------------------------------------------------
INITTASK taskClass = cacheperf.CachePerfClient taskMethod = addDataHostTask
         hydra.ConfigPrms-cacheConfig = cache
         hydra.ConfigPrms-regionConfig = region
         threadGroups = extra;

//------------------------------------------------------------------------------
// Disconnect then reinitialize the extra vm to cause gii with disk correlation.
// Targeting the extra vm means we will always be cycling a vm with the same
// amount of data from run to run. If we chose to bounce a dataStore vm we might
// randomly choose one that has one more bucket than another vm would have in a 
// different run.
//------------------------------------------------------------------------------
INITTASK taskClass = cacheperf.CachePerfClient taskMethod = shutDownDataHostTask
         threadGroups = extra;

//------------------------------------------------------------------------------
// Put task causes more work for recovery and stresses it
// Should cause a DeltaGII
// This randomly puts 20%  of the region
//------------------------------------------------------------------------------
INITTASK taskClass = cacheperf.CachePerfClient taskMethod = putDataTask
         cacheperf.CachePerfPrms-keyAllocation = sameKeysRandomWrap
         cacheperf.CachePerfPrms-taskTerminatorFrequency = 1 iterations
         cacheperf.CachePerfPrms-taskTerminatorMethod = terminateOnNumOperations
         cacheperf.CachePerfPrms-warmupTerminatorMethod = null // no warmup needed, destroys are to make recovery harder for the product
                                                               // not for performance purposes
         cacheperf.CachePerfPrms-numOperations = fcn ((int)(Math.ceil((${maxKeys} * ${peerHosts} * ${peerVMsPerHost} * 0.2) /
                                                      (${peerHosts} * ${peerVMsPerHost})))) ncf
         threadGroups = peer
         batch
         ;

INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = openStatisticsTask
          threadGroups = extra;
INITTASK taskClass = cacheperf.CachePerfClient taskMethod = timedOpenCacheTask
         hydra.ConfigPrms-cacheConfig = cache
         hydra.ConfigPrms-regionConfig = region
         threadGroups = extra;

//------------------------------------------------------------------------------
// Destroy tasks cause more work for recovery and stress it
// This randomly destroys 20%  of the region
//------------------------------------------------------------------------------

INITTASK taskClass = cacheperf.CachePerfClient taskMethod = destroyDataTask
         cacheperf.CachePerfPrms-keyAllocation = sameKeysRandomWrap
         cacheperf.CachePerfPrms-taskTerminatorFrequency = 1 iterations
         cacheperf.CachePerfPrms-taskTerminatorMethod = terminateOnNumOperations
         cacheperf.CachePerfPrms-warmupTerminatorMethod = null // no warmup needed, destroys are to make recovery harder for the product
                                                               // not for performance purposes
         cacheperf.CachePerfPrms-numOperations = fcn ((int)(Math.ceil((${maxKeys} * ${peerHosts} * ${peerVMsPerHost} * 0.2) /
                                                      (${peerHosts} * ${peerVMsPerHost})))) ncf
         threadGroups = peer
         batch
         ;

//------------------------------------------------------------------------------
// Force a gc in all vms.
//------------------------------------------------------------------------------

INITTASK taskClass = cacheperf.CachePerfClient taskMethod = doGC
         threadGroups = peer, extra;

//------------------------------------------------------------------------------
// Run compaction on the disk files.
//------------------------------------------------------------------------------

INITTASK taskClass = cacheperf.CachePerfClient taskMethod = compactionTask
         threadGroups = peer, extra;

//------------------------------------------------------------------------------
// Task to stop and restart all peer vms
//------------------------------------------------------------------------------

TASK taskClass = cacheperf.CachePerfClient taskMethod = stopOtherVMs
     cacheperf.CachePerfPrms-useShutDownAllMembers = ${useShutDownAllMembers}
     weight = 10 // other tasks, if any, will run first
     maxTimesToRun = 1
     threadGroups = controller;

//------------------------------------------------------------------------------
// Task to do recovery in a newly started vm; note that runMode is dynamic
//------------------------------------------------------------------------------

INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = openStatisticsTask
          threadGroups = peer, extra
          runMode = dynamic
          ;
INITTASK taskClass = cacheperf.CachePerfClient taskMethod = timedOpenCacheTask
         hydra.ConfigPrms-cacheConfig = cache
         hydra.ConfigPrms-regionConfig = region
         threadGroups = peer, extra
         runMode = dynamic
         ;

//------------------------------------------------------------------------------
// Performance
//------------------------------------------------------------------------------

perffmwk.PerfReportPrms-statisticsSpecification = $JTESTS/cacheperf/gemfire/specs/cacheOpens.spec;

//------------------------------------------------------------------------------
// VM args
//------------------------------------------------------------------------------
hydra.VmPrms-extraVMArgs =
  fcn "hydra.TestConfigFcns.duplicateString(\"-Dnone\", ${locatorHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms${heapMB}m -Xmx${heapMB}m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+DisableExplicitGC -XX:CMSInitiatingOccupancyFraction=33\", ${peerHosts}, true)" ncf,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms${heapMB}m -Xmx${heapMB}m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+DisableExplicitGC -XX:CMSInitiatingOccupancyFraction=33\", ${extraHosts}, true)" ncf,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms20m -Xmx20m\", ${controllerHosts}, true)" ncf
  ;


hydra.VmPrms-extraVMArgs += "-Dgemfire.disk.recoverValues=false -XX:-DisableExplicitGC";

hydra.Prms-maxResultWaitSec = 604800; // one week 

// Test is configured for a compact step after the destroy step.
// These settings allow an explicit call to force compaction. 
hydra.DiskStorePrms-autoCompact = false;
hydra.DiskStorePrms-allowForceCompaction = true;
hydra.DiskStorePrms-compactionThreshold = 100; 

cacheperf.CachePerfPrms-objectType = objects.ArrayOfByte;
