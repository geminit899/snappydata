include $JTESTS/hydraconfig/hydraparams1.inc;
include $JTESTS/hydraconfig/performance.inc;
include $JTESTS/hydraconfig/topology_p2p_2_locator.inc;

hydra.Prms-testDescription = "Peer feeds create partitioned data on peer datahosts then datahosts bounce and recover redundancy.";
hydra.Prms-testRequirement = "Throughput scales linearly.";

hydra.Prms-totalTaskTimeSec = 999999; // terminates on num recoveries
hydra.Prms-maxResultWaitSec = fcn ${batchSeconds} + 120 ncf;

//------------------------------------------------------------------------------
// Threadgroups
//------------------------------------------------------------------------------

THREADGROUP locator
  totalThreads = fcn
                 ${locatorHosts} * ${locatorVMsPerHost} * ${locatorThreadsPerVM}
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"locator\", ${locatorHosts}, true)"
                 ncf;
THREADGROUP data
    totalThreads = fcn
                   ${dataHosts} * ${dataVMsPerHost} * ${dataThreadsPerVM}
                   ncf
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"data\", ${dataHosts}, true)"
                   ncf;
THREADGROUP feed
    totalThreads = fcn
                   ${feedHosts} * ${feedVMsPerHost} * ${feedThreadsPerVM}
                   ncf
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"feed\", ${feedHosts}, true)"
                   ncf;

//------------------------------------------------------------------------------
// Tasks
//------------------------------------------------------------------------------

// run this task first
INITTASK  taskClass = cacheperf.comparisons.parReg.recovery.PRObserver
          taskMethod = installObserverHook
          runMode = dynamic
          threadGroups = data
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = createAndStartLocatorTask    
          threadGroups = locator
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = openStatisticsTask
          threadGroups = data, feed, locator
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = openCacheTask
          hydra.ConfigPrms-cacheConfig = data
          hydra.ConfigPrms-regionConfig = data
          threadGroups = data
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = openCacheTask
          hydra.ConfigPrms-cacheConfig = feed
          hydra.ConfigPrms-regionConfig = feed
          threadGroups = feed
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = assignBucketsTask
          threadGroups = data
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = createDataTask
          batch
          cacheperf.CachePerfPrms-keyAllocation = ownKeysChunked
          cacheperf.CachePerfPrms-warmupTerminatorMethod = terminateOnTrimIterations
          cacheperf.CachePerfPrms-warmupTerminatorFrequency = 1 iterations
          cacheperf.CachePerfPrms-trimIterations = 0
          cacheperf.CachePerfPrms-taskTerminatorMethod = terminateOnMaxKey
          cacheperf.CachePerfPrms-taskTerminatorFrequency = 1 iterations
          threadGroups = feed
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = restartDataHostTask
          hydra.ConfigPrms-cacheConfig = data
          hydra.ConfigPrms-regionConfig = data
          runMode = dynamic
          threadGroups = data
          ;
TASK      taskClass = cacheperf.CachePerfClient taskMethod = bounceTask
          cacheperf.CachePerfPrms-waitForTrimSignal = true
          cacheperf.CachePerfPrms-maxExecutions = ${maxBounces}
          maxThreads = 1 // one bounce at a time 
          threadGroups = locator
          ;
CLOSETASK taskClass = cacheperf.CachePerfClient taskMethod = closeCacheTask
          threadGroups = data, feed
          ;
CLOSETASK taskClass = cacheperf.CachePerfClient taskMethod = closeStatisticsTask
          threadGroups = data, feed
          ;
CLOSETASK taskClass = cacheperf.CachePerfClient taskMethod = reportTrimIntervalsTask
          threadGroups = data, feed, locator
          ;
CLOSETASK taskClass = cacheperf.CachePerfClient taskMethod = shutDownDataHostTask
          runMode = dynamic
          threadGroups = data
          ;

//------------------------------------------------------------------------------

cacheperf.CachePerfPrms-clientNameToBounce = data*;
cacheperf.CachePerfPrms-restartWaitSec = 0;
cacheperf.CachePerfPrms-sleepMs = 0;
cacheperf.CachePerfPrms-useMeanKill = true;
cacheperf.CachePerfPrms-waitForRecovery = true;

//------------------------------------------------------------------------------

cacheperf.CachePerfPrms-batchTerminatorMethod     = terminateOnBatchSeconds;
cacheperf.CachePerfPrms-batchSeconds              = ${batchSeconds};
cacheperf.CachePerfPrms-warmupTerminatorMethod    = terminateAndSignalOnTrimSeconds;
cacheperf.CachePerfPrms-warmupTerminatorFrequency = 5 seconds;
cacheperf.CachePerfPrms-trimSeconds               = ${trimSeconds};
cacheperf.CachePerfPrms-taskTerminatorMethod      = terminateOnSignal;
cacheperf.CachePerfPrms-taskTerminatorFrequency   = 5 seconds;

//------------------------------------------------------------------------------
// Cache and region
//------------------------------------------------------------------------------

hydra.CachePrms-names               = data      feed;

hydra.RegionPrms-names              = data      feed;
hydra.RegionPrms-dataPolicy         = partition partition;
hydra.RegionPrms-partitionName      = data      feed;
hydra.RegionPrms-scope              = default   default;

hydra.PartitionPrms-names           = data      feed;
hydra.PartitionPrms-localMaxMemory  = default   0;
hydra.PartitionPrms-redundantCopies = ${redundantCopies};
hydra.PartitionPrms-totalNumBuckets = fcn ${bucketsPerDatahost} * ${dataHosts} * ${dataVMsPerHost} ncf;

//------------------------------------------------------------------------------
// Data
//------------------------------------------------------------------------------

cacheperf.CachePerfPrms-maxKeys    = fcn ${maxKeys} * ${dataHosts} * ${dataVMsPerHost} ncf;
cacheperf.CachePerfPrms-keyAllocationChunkSize = fcn ${bucketsPerDatahost} * ${dataHosts} * ${dataVMsPerHost} ncf;
cacheperf.CachePerfPrms-keyType    = java.lang.Long;
cacheperf.CachePerfPrms-objectType = objects.ArrayOfByte;
objects.ArrayOfBytePrms-size       = ${objectSize};

//------------------------------------------------------------------------------
// Versioning (allows running current tests against old builds)
//------------------------------------------------------------------------------

hydra.ClientPrms-versionNames = version;

hydra.VersionPrms-names   = version;
hydra.VersionPrms-version = ${version};

//------------------------------------------------------------------------------
// Performance
//------------------------------------------------------------------------------

hydra.VmPrms-extraVMArgs =
  fcn "hydra.TestConfigFcns.duplicateString(\"-Dnone\", ${locatorHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms${heapMB}m -Xmx${heapMB}m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+DisableExplicitGC -XX:CMSInitiatingOccupancyFraction=33\", ${dataHosts}, true)" ncf
  ,
  fcn "hydra.TestConfigFcns.duplicateString(\"-Xms250m -Xmx250m\", ${feedHosts}, true)" ncf
  ;

hydra.GemFirePrms-conserveSockets = ${conserveSockets};

perffmwk.PerfReportPrms-statisticsSpecification = $JTESTS/cacheperf/comparisons/parReg/recovery/recovery.spec;
