include $JTESTS/hydraconfig/hydraparams1.inc;
include $JTESTS/hydraconfig/performance.inc;
include $JTESTS/hydraconfig/gemfireprms.inc;

hydra.Prms-testDescription = "
    Publisher:  ${pubHosts} hosts with ${pubVMs} VMs per host and ${pubThreads} threads per VM
                ${workVMs} VM per host with ${workThreads} threads per VM doing non-GemFire work.

    Subscriber: ${subHosts} hosts with ${subVMs} VMs per host and ${subThreads} threads per VM
                ${workVMs} VM per host with ${workThreads} threads per VM doing non-GemFire work.
    

    Publishers and Subscribers run in different VMs.

    One publisher running on one host with subscribers on separate hosts are connected via GFE.
    All system members use controlled reliable multicast.  As an initialization step, the publisher
    creates 100K entries in a single region.  During the work phase of the test, the publisher performs
    updates on random keys.

    The test is run in two versions, throughput and latency.  The subscribers have listeners that either
    do no work (throughput test) or record latency of event delivery and the number of spikes > 10 ms
    (latency test).  In the latency tests, the publishers' rate of updates is optionally throttled using
    a configurable sleep time, set during the tuning phase of test development for ~2000 updates/second
    per publisher.  After the tuning phase, the throttle level will not be changed in order to maintain
    testing consistency.

   In both tests, the publisher and subscriber nodes have a task performing non-GemFire work
   (mathematicl computation, string manipulations, sleeps) during the update portion of the test.
   This task can be turned off (set cacheperf.CachePerfPrms-doWorkTask = false) but its load is not 
   configurablee.  Normal runs of this test always have this task running.    
";
hydra.Prms-testRequirement = "
   Throughput degradation from 2 to 16 subscribers is less than 20%
";

hydra.HostPrms-names =
  fcn "hydra.TestConfigFcns.generateNames ( \"hostPub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"hostSub\", ${subHosts} )" ncf
  ;

hydra.VmPrms-names =
  fcn "hydra.TestConfigFcns.generateNames ( \"vmPub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"vmWorkPub\", ${pubHosts} )" ncf  
  fcn "hydra.TestConfigFcns.generateNames ( \"vmSub\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"vmWorkSub\", ${subHosts} )" ncf
  ;

hydra.VmPrms-hostNames =
  fcn "hydra.TestConfigFcns.generateNames ( \"hostPub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"hostPub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"hostSub\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"hostSub\", ${subHosts} )" ncf
  ;

hydra.GemFirePrms-names =
  fcn "hydra.TestConfigFcns.generateNames ( \"gemfirePub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"gemfirePubWork\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"gemfireSub\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"gemfireSubWork\", ${subHosts} )" ncf
  ;

hydra.GemFirePrms-hostNames =
  fcn "hydra.TestConfigFcns.generateNames ( \"hostPub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"hostPub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"hostSub\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"hostSub\", ${subHosts} )" ncf
  ;

hydra.GemFirePrms-distributedSystem =
  fcn "hydra.TestConfigFcns.duplicate ( \"testDs\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate ( \"loner\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate ( \"testDs\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate ( \"loner\", ${subHosts} )" ncf
  ;

hydra.ClientPrms-names =
  fcn "hydra.TestConfigFcns.generateNames ( \"pub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"pubwork\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"sub\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"subwork\", ${subHosts} )" ncf
  ;

hydra.ClientPrms-gemfireNames =
  fcn "hydra.TestConfigFcns.generateNames ( \"gemfirePub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"gemfirePubWork\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"gemfireSub\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"gemfireSubWork\", ${subHosts} )" ncf
  ;

hydra.ClientPrms-vmQuantities =
  fcn "hydra.TestConfigFcns.duplicate( \"${pubVMs}\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate( \"${workVMs}\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate( \"${subVMs}\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate( \"${workVMs}\", ${subHosts} )" ncf
  ;

hydra.ClientPrms-vmThreads =
  fcn "hydra.TestConfigFcns.duplicate( \"${pubThreads}\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate( \"${workThreads}\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate( \"${subThreads}\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.duplicate( \"${workThreads}\", ${subHosts} )" ncf
  ;

hydra.ClientPrms-vmNames =
  fcn "hydra.TestConfigFcns.generateNames ( \"vmPub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"vmWorkPub\", ${pubHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"vmSub\", ${subHosts} )" ncf
  fcn "hydra.TestConfigFcns.generateNames ( \"vmWorkSub\", ${subHosts} )" ncf
  ;

THREADGROUP pubwork 
totalThreads = fcn ${workThreads} * ${pubHosts} ncf
totalVMs = fcn ${workVMs} * ${pubHosts} ncf
clientNames = fcn "hydra.TestConfigFcns.generateNames( \"pubwork\", ${pubHosts}, true )" ncf
;

THREADGROUP subwork 
totalThreads = fcn ${workThreads} * ${subHosts} ncf
totalVMs = fcn ${workVMs} * ${subHosts} ncf
clientNames = fcn "hydra.TestConfigFcns.generateNames( \"subwork\", ${subHosts}, true )" ncf
;

THREADGROUP pub
totalThreads = fcn ${pubThreads} * ${pubVMs} * ${pubHosts} ncf
totalVMs = fcn ${pubVMs} * ${pubHosts} ncf
clientNames = fcn "hydra.TestConfigFcns.generateNames( \"pub\", ${pubHosts}, true )" ncf
;
THREADGROUP sub
totalThreads = fcn ${subThreads} * ${subVMs} * ${subHosts} ncf
totalVMs = fcn ${subVMs} * ${subHosts} ncf
clientNames = fcn "hydra.TestConfigFcns.generateNames( \"sub\", ${subHosts}, true )" ncf
;

INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = openStatisticsTask
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = openCacheTask
          hydra.ConfigPrms-cacheConfig = pub
          hydra.ConfigPrms-regionConfig = pub
          threadGroups = pub
          ;
INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = openCacheTask
          hydra.ConfigPrms-cacheConfig = sub
          hydra.ConfigPrms-regionConfig = sub
          threadGroups = sub
          ;

hydra.CachePrms-names             = pub     sub;
hydra.RegionPrms-names            = pub     sub;
hydra.RegionPrms-dataPolicy       = default replicated;
hydra.RegionPrms-cacheListeners   = none,   ${listenerClass};
hydra.RegionPrms-scope            = ${scope};
hydra.RegionPrms-multicastEnabled = ${multicastEnabled};

hydra.GemFirePrms-conserveSockets = true;
hydra.GemFirePrms-enableMcast = ${enableMcast};
hydra.GemFirePrms-mcastTtl = 1;
hydra.GemFirePrms-useLocator = ${useLocator};

INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = validateExpectedMembersTask
          threadGroups = pub, sub
          ;

INITTASK  taskClass = cacheperf.CachePerfClient taskMethod = createDataTask
          threadGroups = pub
          batch
          cacheperf.CachePerfPrms-keyAllocation = ownKeys
          cacheperf.CachePerfPrms-warmupTerminatorMethod = none
          cacheperf.CachePerfPrms-taskTerminatorMethod = terminateOnMaxKey
          cacheperf.CachePerfPrms-taskTerminatorFrequency = 1 iterations;
          ;

INITTASK  taskClass = util.CpuLoad taskMethod = initNonGemFireWorkTask
          threadGroups = pubwork
	  util.CpuLoadPrms-sleepMsNonGemFire = ${pubWorkSleepMs}
          util.CpuLoadPrms-cpuLoadMin = fcn ${pubWorkLoad} - 5 ncf
          util.CpuLoadPrms-cpuLoadMax = fcn ${pubWorkLoad} + 5 ncf
          ;

INITTASK  taskClass = util.CpuLoad taskMethod = initNonGemFireWorkTask
          threadGroups = subwork
          util.CpuLoadPrms-sleepMsNonGemFire = ${subWorkSleepMs}
          util.CpuLoadPrms-cpuLoadMin = fcn ${pubWorkLoad} + 10 ncf
          util.CpuLoadPrms-cpuLoadMax = fcn ${pubWorkLoad} + 20 ncf 
          ;

TASK      taskClass = cacheperf.CachePerfClient taskMethod = putDataTask
          threadGroups = pub
          cacheperf.CachePerfPrms-keyAllocation = ownKeysRandomWrap
          cacheperf.CachePerfPrms-sleepBeforeOp = ${sleepBeforeOp}
          cacheperf.CachePerfPrms-sleepMs = range ${putSleepMsMin} ${putSleepMsMax} egnar
          cacheperf.CachePerfPrms-sleepOpCount = ${sleepOpCount}
          ;

TASK      taskClass = util.CpuLoad taskMethod = doNonGemFireWorkTask
          threadGroups = pubwork
	  util.CpuLoadPrms-sleepMsNonGemFire = ${pubWorkSleepMs}
          MaxTimesToRun = 1
          ;

TASK      taskClass = util.CpuLoad taskMethod = doNonGemFireWorkTask
          threadGroups = subwork
          util.CpuLoadPrms-sleepMsNonGemFire = ${subWorkSleepMs}
          MaxTimesToRun = 1
          ;

CLOSETASK taskClass = cacheperf.CachePerfClient taskMethod = closeCacheTask
          ;
CLOSETASK taskClass = cacheperf.CachePerfClient taskMethod = closeStatisticsTask
          ;
CLOSETASK taskClass = cacheperf.CachePerfClient taskMethod = reportTrimIntervalsTask
          ;

cacheperf.CachePerfPrms-batchTerminatorMethod = terminateOnBatchSeconds;
cacheperf.CachePerfPrms-batchSeconds = 30;

cacheperf.CachePerfPrms-warmupTerminatorMethod = terminateOnTrimSeconds;
cacheperf.CachePerfPrms-warmupTerminatorFrequency = 30 seconds;
cacheperf.CachePerfPrms-trimSeconds = 180;

cacheperf.CachePerfPrms-taskTerminatorMethod = terminateOnTotalSeconds;
cacheperf.CachePerfPrms-taskTerminatorFrequency = 180 seconds;
cacheperf.CachePerfPrms-workSeconds = 420;

// clients start first and are not stopped until they run for totalTaskTimeSec
hydra.Prms-totalTaskTimeSec = 999999;  // see terminators
util.CpuLoadPrms-workSec    = 780; // set to exceed max amt of time putData will run
hydra.Prms-maxResultWaitSec = 960; // set to exceed time doNonGemfireWorkTask

hydra.VmPrms-extraVMArgs += "-Xms512m -Xmx512m";

cacheperf.CachePerfPrms-syncSleepMs    = 4000;
cacheperf.CachePerfPrms-maxKeys        = 102400; 
cacheperf.CachePerfPrms-objectType     = objects.PSTObject;

objects.ArrayOfBytePrms-size = ${byteArraySize};

perffmwk.PerfReportPrms-statisticsSpecification = $JTESTS/cacheperf/comparisons/peer2peer/peer2peer.spec;

util.RandomValuesPrms-stringSize = 10000;
util.RandomValuesPrms-objectType = "String";
util.RandomValuesPrms-keyType = "String";
util.RandomValuesPrms-valueType = "String";
util.RandomValuesPrms-setElementType = "String";
util.RandomValuesPrms-borderCasePercentage = 1;
