hydra.Prms-testDescription = "
This test creates a partitioned region without custom parititioning , then does various entry operations(using function execution)
 on a known key range. Values are checked for correctness in the close task.
 This test is with client server configuration.
";

INCLUDE $JTESTS/hydraconfig/hydraparams1.inc;
INCLUDE $JTESTS/hydraconfig/topology_hct.inc;
INCLUDE $JTESTS/parReg/execute/concKnownKeysFuncExec.inc;

// This bridge flavor of concKnownKeysHA is a little different than
// the non-bridge flavor. In the non-bridge flavor, accessor VMs 
// (which correlate to edge clients) have a "handle" to the PR, and
// even though we don't store any data in the accessor, logically
// all the keys exist. In the bridge flavor, the edge clients, which
// have local regions, need to also have the keys in order for the ops
// task to do its work. For this reason, the edge clients must also
// keep all keys and values in it's locally scoped region, thus its
// vm must be as large as the data store (ie server) vms.


// this test uses tasks from getInitialImage tests to load the region,
// do operations and verify the region contents 

// one verify thread per datastore (but not accessor) vm to verify each vm's view
THREADGROUP verifyThreads 
            totalThreads = fcn "(${bridgeHosts} * ${bridgeVMsPerHost})" ncf
            totalVMs     = fcn "(${bridgeHosts} * ${bridgeVMsPerHost})" ncf
            clientNames  = fcn "hydra.TestConfigFcns.generateNames(\"bridge\", 
                                ${bridgeHosts} * ${bridgeVMsPerHost}, true)" ncf;

// accessorThreads are all threads in the accessor VMs 
THREADGROUP accessorVMThreads 
            totalThreads = fcn "(${edgeHosts} * ${edgeVMsPerHost} * ${edgeThreadsPerVM})" ncf
            totalVMs     = fcn "(${edgeHosts} * ${edgeVMsPerHost})" ncf
            clientNames  = fcn "hydra.TestConfigFcns.generateNames(\"edge\", 
                                ${edgeHosts} * ${edgeVMsPerHost}, true)" ncf;

// dataStoreThreads are all threads in the dataStore VMs minus 
// the one thread per dataStore VM for the verifyThreads
THREADGROUP dataStoreVMThreads 
            totalThreads = fcn (${bridgeHosts} * ${bridgeVMsPerHost} * ${bridgeThreadsPerVM}) 
                               - (${bridgeHosts} * ${bridgeVMsPerHost}) ncf  
            totalVMs     = fcn ${bridgeHosts} * ${bridgeVMsPerHost} ncf
            clientNames  = fcn "hydra.TestConfigFcns.generateNames(\"bridge\", 
                                ${bridgeHosts} * ${bridgeVMsPerHost}, true)" ncf;


hydra.RegionPrms-names              = accessorRegion         dataStoreRegion;
hydra.RegionPrms-regionName         = testRegion             testRegion;
hydra.RegionPrms-scope              = local                  default;
hydra.RegionPrms-partitionName      = none                   dataStorePR;
hydra.RegionPrms-poolName           = edgeDescript           none;
hydra.RegionPrms-dataPolicy         = normal                 partition;
hydra.RegionPrms-partitionName      = none                   pr;
hydra.RegionPrms-cacheListeners 	= util.SilenceListener,	 none;

hydra.PartitionPrms-names           = pr;
hydra.PartitionPrms-localMaxMemory  = default;
hydra.PartitionPrms-redundantCopies = ${redundantCopies};
 
// define the edge clients
hydra.PoolPrms-names                  = edgeDescript;
hydra.PoolPrms-minConnections         = 2;
hydra.PoolPrms-subscriptionEnabled    = true;
hydra.PoolPrms-threadLocalConnections = false;
hydra.PoolPrms-readTimeout            = 3600000; // hydra will hang first
hydra.PoolPrms-subscriptionRedundancy = ${redundancy};
hydra.PoolPrms-pingInterval           = 400;
hydra.PoolPrms-loadConditioningInterval = 5000;

// define the bridge servers
hydra.BridgePrms-names                = bridge;

	
parReg.ParRegPrms-numberOfDataStore = fcn "(${bridgeHosts} * ${bridgeVMsPerHost})" ncf;
