hydra.Prms-testDescription = "This test is for verifying FPR behavior when a node gets recycled. This test verifies the following
(1) When a node with primary partition goes away, the existing primary bucket of another member with the same partition as secondary becomes primary.
(2) In (1), all bucket of the primary partition will be hosted by only one (new)member.
(3) When a primary or secondary partition goes down, the redundantCopies of its buckets are not satisfied by using members that does not mention the partition.
(4) When a primary partition comes back, then it will grab the buckets and grab back the primary status of the buckets of its primary partition.
(5) When a secondary partition comes back, it will grab the bucket to help satisfy redundantCopies.

NOTE: This test will work only if there is only one primary partition per member.
";

INCLUDE $JTESTS/hydraconfig/hydraparams1.inc;
INCLUDE $JTESTS/hydraconfig/topology_p2p_2.inc;                                          

hydra.VmPrms-extraVMArgs   = 
                             fcn "hydra.TestConfigFcns.duplicate
                                  (\"-Xmx128m \", ${${A}Hosts}, true)"
                              ncf
                              ,
                             fcn "hydra.TestConfigFcns.duplicate
                                  (\"-Xms512m -Xmx512m \", ${${B}Hosts}, true)"
                             ncf;


// this test uses tasks similar to getInitialImage tests to load the region,
// do operations and verify the region contents 

// one verify thread per accessor and datastore vm to verify each vm's view
THREADGROUP verifyThreads 
            totalThreads = fcn "(${${A}Hosts} * ${${A}VMsPerHost}) +
                                (${${B}Hosts} * ${${B}VMsPerHost})" ncf
            totalVMs     = fcn "(${${A}Hosts} * ${${A}VMsPerHost}) +
                                (${${B}Hosts} * ${${B}VMsPerHost})" ncf;

// accessorThreads are all threads in the accessor VMs minus 1 thread for the controller
// thread, minus one thread per accessor VM for the verifyThreads
THREADGROUP accessorVMThreads 
            totalThreads = fcn (${${A}Hosts} * ${${A}VMsPerHost} * ${${A}ThreadsPerVM})
                               - (${${A}Hosts} * ${${A}VMsPerHost}) ncf  
            totalVMs     = fcn ${${A}Hosts} * ${${A}VMsPerHost} ncf
            clientNames  = fcn "hydra.TestConfigFcns.generateNames(\"${A}\", 
                                ${${A}Hosts} * ${${A}VMsPerHost}, true)" ncf;

// dataStoreThreads are all threads in the dataStore VMs minus 
// the one thread per dataStore VM for the verifyThreads
THREADGROUP dataStoreVMThreads 
            totalThreads = fcn (${${B}Hosts} * ${${B}VMsPerHost} * ${${B}ThreadsPerVM}) 
                               - (${${B}Hosts} * ${${B}VMsPerHost}) ncf  
            totalVMs     = fcn ${${B}Hosts} * ${${B}VMsPerHost} ncf
            clientNames  = fcn "hydra.TestConfigFcns.generateNames(\"${B}\", 
                                ${${B}Hosts} * ${${B}VMsPerHost}, true)" ncf;

STARTTASK    taskClass     = parReg.fixedPartitioning.FixedPartitioningTest  taskMethod = StartTask_initialize;

INITTASK     taskClass     = parReg.fixedPartitioning.FixedPartitioningTest  taskMethod = HydraTask_p2p_dataStoreInitialize
	     hydra.ConfigPrms-regionConfig = dataStoreRegion
	     hydra.ConfigPrms-cacheConfig = cache             
             threadGroups = dataStoreVMThreads
             runMode = always
             SEQUENTIAL;

INITTASK     taskClass     = parReg.fixedPartitioning.FixedPartitioningTest  taskMethod = HydraTask_p2p_accessorInitialize
	     hydra.ConfigPrms-regionConfig = accessorRegion
	     hydra.ConfigPrms-cacheConfig = cache
             threadGroups = accessorVMThreads;

INITTASK     taskClass     = parReg.fixedPartitioning.FixedPartitioningTest  taskMethod = HydraTask_loadRegions
             threadGroups = accessorVMThreads
             batch
             ;            

INITTASK     taskClass     = parReg.fixedPartitioning.FixedPartitioningTest  taskMethod  = HydraTask_updateBBWithPartitionInfo
             threadGroups = verifyThreads; 
                
INITTASK    taskClass     = parReg.fixedPartitioning.FixedPartitioningTest  taskMethod  = HydraTask_verifyFixedPartitioning
             threadGroups = verifyThreads
             ;			 
			 
TASK    taskClass     = parReg.fixedPartitioning.FixedPartitioningTest  taskMethod  = HydraTask_recycleClientAndValidateBehavior
             threadGroups = accessorVMThreads
             maxThreads = 1
			 ;  
			 
CLOSETASK    taskClass     = parReg.fixedPartitioning.FixedPartitioningTest  taskMethod  = HydraTask_verifyFixedPartitioning
             threadGroups = verifyThreads
             ;			 			                                                                       

hydra.RegionPrms-names          = dataStoreRegion  accessorRegion;
hydra.RegionPrms-regionName     = testRegion       testRegion;
hydra.RegionPrms-cacheListeners = util.SilenceListener,  util.SilenceListener;
hydra.RegionPrms-scope          = default          default;
hydra.RegionPrms-dataPolicy     = partition        partition;
hydra.RegionPrms-partitionName  = prDS             prAcc;                                                               
                
hydra.PartitionPrms-names              = prDS               prAcc;
hydra.PartitionPrms-redundantCopies    = ${redundantCopies};
hydra.PartitionPrms-localMaxMemory     = default            0;
hydra.PartitionPrms-totalNumBuckets    = 113                113;
hydra.PartitionPrms-partitionResolver  = parReg.fixedPartitioning.NodePartitionResolver  parReg.fixedPartitioning.NodePartitionResolver;
hydra.PartitionPrms-fixedPartitionName = quarters           none;

hydra.FixedPartitionPrms-names            = quarters;
hydra.FixedPartitionPrms-partitionNames   = Quarter1 Quarter2 Quarter3 Quarter4;
hydra.FixedPartitionPrms-partitionBuckets = 1         3        1        3;
hydra.FixedPartitionPrms-datastores       = fcn "(${${B}Hosts} * ${${B}VMsPerHost})" ncf;

hydra.Prms-totalTaskTimeSec = 400; 
hydra.Prms-maxResultWaitSec = 400;

util.TestHelperPrms-minTaskGranularitySec = 60;

getInitialImage.InitImagePrms-numKeys = ${numKeys};
// numNewKeys is 10% of the total number of keys
getInitialImage.InitImagePrms-numNewKeys = fcn "${numKeys} * 0.1" ncf;
getInitialImage.InitImagePrms-useCacheLoader=false;

INCLUDE $JTESTS/util/randomValues.inc;
util.RandomValuesPrms-objectType = byte[];
util.RandomValuesPrms-elementSize = ${byteArraySize};
util.ValueHolderPrms-useExtraObject = true;

hydra.GemFirePrms-conserveSockets = ONEOF true false FOENO;

hydra.CachePrms-names           = cache;
hydra.CachePrms-searchTimeout   = 600;
           
parReg.ParRegPrms-partitionResolverData = BB;
parReg.ParRegPrms-isWithRoutingResolver = true;

// NumVms to stop is 1 for this test
//parReg.ParRegPrms-numVMsToStop = RANGE 1 ${numVMsToStop} EGNAR;
parReg.ParRegPrms-stopModes = ONEOF MEAN_EXIT MEAN_KILL NICE_EXIT NICE_KILL FOENO;
parReg.ParRegPrms-highAvailability = true;
