hydra.Prms-testRequirement = "Verify view of tx and non-tx threads.  Verify that cacheListeners, cacheWriters and TransactionListeners are invoked as expected";
hydra.Prms-testDescription = "In this serial round-robin test, one (edge client) thread starts a transaction and does multiple operations on colocated entries in partitionedRegions in the servers.  The test verifies that the cacheWriter is verified inline with the operations.  In addition, each VM verifies it's view of the data for the affected entries with the txThread verifying it sees the TxState and the other threads verifying that those changes are not visible until commit time.  Cache and TxListeners are also invoked and the contents of those events validated.";

include $JTESTS/hydraconfig/hydraparams1.inc;
include $JTESTS/hydraconfig/topology_hct.inc;
include $JTESTS/util/randomValues.inc;

THREADGROUP bridge
  totalThreads = fcn
                 ${bridgeHosts} * ${bridgeVMsPerHost} * ${bridgeThreadsPerVM}
                 ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames
                      (\"bridge\", ${bridgeHosts}, true)"
                 ncf;

THREADGROUP txThread totalThreads = 1  totalVMs = 1 clientNames=edge1;
THREADGROUP killer   totalThreads = 1  totalVMs = 1 clientNames=edge1;
THREADGROUP validator totalThreads = 1 totalVMs = 1 clientNames=edge2;
// extra threads are default (edge2, edge3)

INITTASK  taskClass   = parReg.tx.PrViewUtil taskMethod = HydraTask_createColocatedRegions
          hydra.ConfigPrms-cacheConfig    = myCache
          hydra.ConfigPrms-regionConfig   = serverRegion
          threadGroups = bridge
          ;

// create entries (so we have partitioned entries to work with)
INITTASK  taskClass   = parReg.tx.PrViewUtil taskMethod = HydraTask_populateRegions
          threadGroups = bridge
          ;

INITTASK  taskClass   = parReg.tx.PrViewUtil taskMethod = HydraTask_dumpLocalKeys
          threadGroups = bridge
          ;

INITTASK  taskClass   = parReg.tx.PrViewUtil taskMethod = HydraTask_startBridgeServer
          hydra.ConfigPrms-bridgeConfig = bridge
          threadGroups = bridge
          ;

// create region tree + registerInterest in ALL_KEYS
INITTASK  taskClass   = parReg.tx.PrViewUtil taskMethod = HydraTask_createClientRegions
          hydra.ConfigPrms-cacheConfig    = myCache
          hydra.ConfigPrms-regionConfig   = edgeRegion
          threadGroups = txThread, default
          ;

// Regions must exist before invoking the ViewTest initialization task
// The txThread needs to know how many listeners will be invoked (depends on PR config in servers)
INITTASK  taskClass   = parReg.tx.ParRegBridgeView taskMethod = HydraTask_initialize
          hydra.ConfigPrms-regionConfig   = serverRegion
          threadGroups = bridge, txThread, default
          ;

TASK      taskClass   = parReg.tx.ParRegBridgeView taskMethod = HydraTask_executeTx
          maxTimesToRun = 2
          threadGroups = txThread
          ;

TASK      taskClass   = parReg.tx.ParRegBridgeView taskMethod = HydraTask_killDelegate
          maxTimesToRun = 1
          threadGroups = killer
          ;

CLOSETASK taskClass   = parReg.tx.ParRegBridgeView taskMethod = HydraCloseTask_validateTxConsistency
          threadGroups = bridge, validator
          ;

// Note that this must come AFTER validateTxConsistency since we'll perform
// operations on the same keys as the previous transaction (which would
// cause the data consistency validation to fail.  Only one VM can execute
// this since we'll be applying the same changes as the original commit
CLOSETASK taskClass   = parReg.tx.ParRegBridgeView taskMethod = HydraCloseTask_verifyResourcesReleased
          threadGroups = validator
          ;

CLOSETASK taskClass   = tx.TxUtil taskMethod = HydraTask_stopBridgeServer
          threadGroups = bridge
          ;

hydra.Prms-totalTaskTimeSec = 300;
hydra.Prms-serialExecution=false;

hydra.log.LogPrms-mergeLogFiles = true;

util.RandomValuesPrms-objectType = byte[];
util.RandomValuesPrms-elementSize = 10000;
util.RandomValuesPrms-objectDepth = 0;
util.ValueHolderPrms-useExtraObject = true;

tx.TxPrms-commitStateTrigger = ${commitStateTrigger};

tx.TxPrms-commitPercentage=${commitPercentage};
tx.TxPrms-maxKeys=10;
tx.TxPrms-numRootRegions=2;
tx.TxPrms-numSubRegions=3;
tx.TxPrms-regionDepth=2;

tx.TxPrms-txListener = parReg.tx.TxViewListener;
tx.TxPrms-txWriter = tx.TxLogWriter;

// todo@lhughes -- add more ops as implemented in pr tx for product
tx.TxPrms-operations = 
          entry-create entry-update entry-destroy entry-inval
          entry-create entry-update entry-destroy entry-inval
          entry-create entry-update entry-destroy entry-inval
          entry-create entry-update entry-destroy entry-inval
          entry-getWithNewKey entry-getWithExistingKey 
          ;

tx.TxPrms-numOps = RANGE 50 100 EGNAR;
tx.TxPrms-updateStrategy = ONEOF useCopyOnRead useCopyHelper FOENO; 

hydra.Prms-useFixedRandomInMaster = true;

hydra.CachePrms-names           = myCache;
hydra.RegionPrms-names          = serverRegion        edgeRegion;
hydra.RegionPrms-regionName     = PartitionedRegion   PartitionedRegion;
hydra.RegionPrms-scope          = default             local;
hydra.RegionPrms-dataPolicy     = partition           normal;
hydra.RegionPrms-poolName       = none                brloader;
hydra.RegionPrms-interestPolicy = default             default;
hydra.RegionPrms-partitionName  = pr                  none;
hydra.RegionPrms-cacheListeners = tx.LogListener;
hydra.RegionPrms-cacheWriter    = tx.LogWriter;
hydra.RegionPrms-cacheLoader    = tx.TxLoader;

hydra.PartitionPrms-names           = pr;
hydra.PartitionPrms-localMaxMemory  = default;
hydra.PartitionPrms-redundantCopies = 1;

hydra.BridgePrms-names          = bridge;

hydra.PoolPrms-names            = brloader;
hydra.PoolPrms-minConnections   = 2;
hydra.PoolPrms-subscriptionEnabled = true;
hydra.PoolPrms-threadLocalConnections = true;
hydra.PoolPrms-subscriptionRedundancy = -1;

tx.TxPrms-txListener = tx.TxViewListener;

parReg.tx.PrTxPrms-numColocatedRegions = 5;
parReg.tx.PrTxPrms-useLocalKeySet = false;  // always use a keySet from a server
parReg.tx.PrTxPrms-killLocalTxVm = false;    // kill tx vm


