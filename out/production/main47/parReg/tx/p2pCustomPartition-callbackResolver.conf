hydra.Prms-testRequirement = "Execute random entry operations on a PartitionedRegion with CustomPartitioning via a CallbackArgument, verify correct partitioning";
hydra.Prms-testDescription = "Peer members execute entry operations on a PR using callbackArguments with the pid of the VM that created the entry.  A CallbackListener installed in each VM verifies that all EntryEvents map to the same routingObject hashcode.  A single VM executes a function across all VMs as a CLOSETASK";

INCLUDE $JTESTS/hydraconfig/hydraparams1.inc;
INCLUDE $JTESTS/hydraconfig/topology_p2p_locator.inc;

//------------------------------------------------------------------------------
// INITIALIZATION WORK 
//------------------------------------------------------------------------------

INITTASK  taskClass = parReg.tx.CustomPartitionTest taskMethod = createLocatorTask
          threadGroups = locator;

INITTASK  taskClass = parReg.tx.CustomPartitionTest taskMethod = startAndConnectLocatorTask
          threadGroups = locator;

INITTASK  taskClass   = parReg.tx.CustomPartitionTest taskMethod  = HydraTask_initialize
          threadGroups = peer;

TASK      taskClass = parReg.tx.CustomPartitionTest taskMethod = HydraTask_doEntryOperations
          threadGroups = peer, verifyThread;

CLOSETASK taskClass = parReg.tx.CustomPartitionTest taskMethod = HydraTask_executeGetAllMembersInDS
          threadGroups = verifyThread;

THREADGROUP locator
    totalThreads = fcn ${locatorHosts} * ${locatorVMsPerHost}
                                       * ${locatorThreadsPerVM}
                   ncf     
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"locator\", ${locatorHosts}, true)"
                   ncf;    
THREADGROUP peer
    totalThreads = fcn ${peerHosts} * ${peerVMsPerHost} * ${peerThreadsPerVM} - 1 ncf     
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"peer\", ${peerHosts}, true)"
                   ncf;

THREADGROUP verifyThread totalThreads = 1;

INCLUDE $JTESTS/util/randomValues.inc;
util.RandomValuesPrms-objectType = byte[];
util.RandomValuesPrms-elementSize = 500;
util.ValueHolderPrms-useExtraObject = true;

util.TestHelperPrms-minTaskGranularitySec = 0;
//util.OperationsClientPrms-entryOperations = ONEOF add add getNew getNew update invalidate get destroy FOENO;
util.OperationsClientPrms-entryOperations = ONEOF putAll add FOENO;
util.OperationsClientPrms-upperThreshold = 1000;
util.OperationsClientPrms-upperThresholdOperations = ONEOF destroy FOENO;
util.OperationsClientPrms-lowerThreshold = 500;
util.OperationsClientPrms-lowerThresholdOperations = ONEOF add getNew FOENO;
util.OperationsClientPrms-lowerThresholdOperations = ONEOF add FOENO;

hydra.GemFirePrms-conserveSockets = true;
hydra.GemFirePrms-stopSystemsAfterTest = true;

hydra.Prms-serialExecution = true;
hydra.Prms-alwaysDoEndTasks = true;
hydra.Prms-clientShutdownHook = parReg.ParRegUtil dumpAllPartitionedRegions;

hydra.Prms-haltIfBadResult = true;

hydra.Prms-totalTaskTimeSec = 300;
hydra.Prms-maxResultWaitSec = 180;
hydra.Prms-haltIfBadResult = true;
hydra.Prms-maxClientShutdownWaitSec = 360;
hydra.Prms-finalClientSleepSec = 60;
hydra.Prms-maxEndTaskResultWaitSec = 1800;

hydra.ConfigPrms-cacheConfig    = myCache;
hydra.ConfigPrms-regionConfig   = myRegion;
hydra.CachePrms-names           = myCache;
hydra.RegionPrms-names          = myRegion   routingRegion;
hydra.RegionPrms-regionName     = TestRegion routingRegion;
hydra.RegionPrms-scope          = default;
hydra.RegionPrms-dataPolicy     = partition;
hydra.RegionPrms-cacheLoader    = parReg.tx.PrTxLoader none;
hydra.RegionPrms-cacheListeners = parReg.tx.CallbackListener, event.LogListener;
hydra.RegionPrms-partitionName  = pr;

hydra.PartitionPrms-names           = pr;
hydra.PartitionPrms-localMaxMemory  = default;
hydra.PartitionPrms-redundantCopies = 0;
hydra.PartitionPrms-totalNumBuckets = fcn ${peerHosts} * ${peerVMsPerHost} ncf;

parReg.tx.PrTxPrms-customPartitionMethod = CallbackResolver;
