hydra.Prms-testRequirement = "Execute random entry operations on a PartitionedRegion wit
h CustomPartitioning via a keyResolver, verify correct partitioning";
hydra.Prms-testDescription = "Peer members execute entry operations on a PR using a KeyResolver (integer portion of keys map to VMs).  CLOSETASKS verify proper targeting of functions and partitioning of data via function execution.";

INCLUDE $JTESTS/hydraconfig/hydraparams1.inc;
INCLUDE $JTESTS/hydraconfig/topology_p2p_locator.inc;

//------------------------------------------------------------------------------
// INITIALIZATION WORK 
//------------------------------------------------------------------------------

INITTASK  taskClass = parReg.tx.CustomPartitionTest taskMethod = createLocatorTask
          threadGroups = locator;

INITTASK  taskClass = parReg.tx.CustomPartitionTest taskMethod = startAndConnectLocatorTask
          threadGroups = locator;

INITTASK  taskClass   = parReg.tx.CustomPartitionTest taskMethod  = HydraTask_initialize
          threadGroups = peer;

TASK      taskClass = parReg.tx.CustomPartitionTest taskMethod = HydraTask_doEntryOperations
          threadGroups = peer, verifyThread;

CLOSETASK taskClass = parReg.tx.CustomPartitionTest taskMethod = HydraTask_executeGetAllMembersInDS
          threadGroups = verifyThread;

CLOSETASK taskClass = parReg.tx.CustomPartitionTest taskMethod = HydraTask_executeVerifyCustomPartitioningFunction
          threadGroups = verifyThread;

THREADGROUP locator
    totalThreads = fcn ${locatorHosts} * ${locatorVMsPerHost}
                                       * ${locatorThreadsPerVM}
                   ncf     
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"locator\", ${locatorHosts}, true)"
                   ncf;    
THREADGROUP peer
    totalThreads = fcn ${peerHosts} * ${peerVMsPerHost} * ${peerThreadsPerVM} - 1 ncf     
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"peer\", ${peerHosts}, true)"
                   ncf;

THREADGROUP verifyThread totalThreads = 1;

INCLUDE $JTESTS/util/randomValues.inc;
util.RandomValuesPrms-objectType = byte[];
util.RandomValuesPrms-elementSize = 500;
util.ValueHolderPrms-useExtraObject = true;

util.TestHelperPrms-minTaskGranularitySec = 60;
//util.OperationsClientPrms-entryOperations = ONEOF add add getNew getNew update invalidate get destroy FOENO;
util.OperationsClientPrms-entryOperations = ONEOF putAll putAll add add getNew getNew update invalidate get putIfAbsent putIfAbsent replace replace remove FOENO;
util.OperationsClientPrms-upperThreshold = 1000;
util.OperationsClientPrms-upperThresholdOperations = ONEOF destroy FOENO;
util.OperationsClientPrms-lowerThreshold = 500;
util.OperationsClientPrms-lowerThresholdOperations = ONEOF add getNew FOENO;
parReg.ParRegPrms-numPutAllNewKeys = 1;
parReg.ParRegPrms-numPutAllExistingKeys = RANGE 1 100 EGNAR;


hydra.GemFirePrms-conserveSockets = true;
hydra.GemFirePrms-stopSystemsAfterTest = true;
hydra.Prms-alwaysDoEndTasks = true;
hydra.Prms-clientShutdownHook = parReg.ParRegUtil dumpAllPartitionedRegions;

hydra.Prms-totalTaskTimeSec = 300;
hydra.Prms-maxResultWaitSec = 180;
hydra.Prms-haltIfBadResult = true;
hydra.Prms-serialExecution = false;
hydra.Prms-maxClientShutdownWaitSec = 360;
hydra.Prms-finalClientSleepSec = 60;
hydra.Prms-maxEndTaskResultWaitSec = 1800;

hydra.ConfigPrms-cacheConfig    = myCache;
hydra.ConfigPrms-regionConfig   = myRegion;
hydra.CachePrms-names           = myCache;
hydra.RegionPrms-names          = myRegion;
hydra.RegionPrms-regionName     = TestRegion;
hydra.RegionPrms-scope          = default;
hydra.RegionPrms-dataPolicy     = partition;
hydra.RegionPrms-partitionName  = pr;

hydra.PartitionPrms-names           = pr;
hydra.PartitionPrms-localMaxMemory  = default;
hydra.PartitionPrms-redundantCopies = 0;
hydra.PartitionPrms-totalNumBuckets = fcn ${peerHosts} * ${peerVMsPerHost} ncf;

parReg.tx.PrTxPrms-customPartitionMethod = KeyResolver;

