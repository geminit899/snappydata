hydra.Prms-testRequirement = "Test HDFS persistent partitioned regions with a variety of operations with careful validation and concurrent execution";

INCLUDE $JTESTS/hydraconfig/hydraparams1.inc;
INCLUDE $JTESTS/hydraconfig/topology_p2p_2_locator.inc;
hydra.Prms-manageLocatorAgents = true; // turn on master-managed locators

STARTTASK taskClass   = hdfs.HDFSUtil
          taskMethod  = configureHadoopTask
          clientNames = locator1
          ;

INITTASK taskClass    = hdfs.HDFSUtil
         taskMethod   = startCluster
         threadGroups = locator
         ;

INITTASK     taskClass   = util.StopStartVMs  taskMethod = StopStart_initTask
             threadGroups = dataStoreThreads, accessorThreads;

INITTASK     taskClass   = util.PRObserver  taskMethod = initialize
             runMode = once
             threadGroups = dataStoreThreads;

INITTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HA_initializeDataStore
             runMode = once
             threadGroups = dataStoreThreads;

INITTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HA_initializeAccessor
             runMode = once
             threadGroups = accessorThreads;

INITTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HA_reinitializeAccessor
             threadGroups = accessorThreads
             runMode = dynamic;

INITTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HA_reinitializeDataStore
             threadGroups = dataStoreThreads
             runMode = dynamic;

INITTASK     taskClass   =  parReg.ParRegTest  taskMethod = HydraTask_waitForMyStartupRecovery
             threadGroups = dataStoreThreads;

// prepare for end task recovery
INITTASK    taskClass   = parReg.ParRegTest  taskMethod = HydraTask_writeDiskDirsToBB
            threadGroups = dataStoreThreads
            runMode = once;

// maxThreads of 1 means only one thread will be running the HAController at a time
TASK         taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HAController
             threadGroups = dataStoreThreads, accessorThreads
             maxThreads = 1;

TASK         taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HADoEntryOps
             threadGroups = dataStoreThreads, accessorThreads;

CLOSETASK    taskClass   = parReg.ParRegUtil  taskMethod = HydraTask_rebalance
             threadGroups = dataStoreThreads;

// recover from HDFS
ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HA_initializeDataStore
            parReg.ParRegPrms-recoverFromDisk = true
            clientNames = fcn "hydra.TestConfigFcns.generateNames (\"dataStore\", ${dataStoreHosts}, true)" ncf;
//start accessors after recovering from disk.            
ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HA_initializeAccessor
            clientNames = fcn "hydra.TestConfigFcns.generateNames (\"accessor\", ${accessorHosts}, true)" ncf;

// validate region contents while redundancy recovery is running,
// wait for redundancy recovery and verify PR internals 
ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_validateRegionContents
            clientNames = dataStore1;
ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_waitForMyStartupRecovery
            clientNames = fcn "hydra.TestConfigFcns.generateNames(\"dataStore\", ${dataStoreHosts}, true)" ncf;
ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_validateInternalPRState
            clientNames = fcn "hydra.TestConfigFcns.generateNames (\"dataStore\", ${dataStoreHosts}, true)" ncf,
                          fcn "hydra.TestConfigFcns.generateNames (\"accessor\", ${accessorHosts}, true)" ncf;

// offline validation and compaction
//ENDTASK     taskClass   = parReg.ParRegTest     taskMethod = HydraTask_disconnect;
//ENDTASK     taskClass   = util.PersistenceUtil  taskMethod = HydraTask_doOfflineValAndCompactionOnce;

// recover from disk
//ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_prepareForRecovery;
//ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HA_initializeDataStore
            //clientNames = fcn "hydra.TestConfigFcns.generateNames (\"dataStore\", ${dataStoreHosts}, true)" ncf;
//start accessors after recovering from disk.            
//ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_HA_initializeAccessor
            //clientNames = fcn "hydra.TestConfigFcns.generateNames (\"accessor\", ${accessorHosts}, true)" ncf;

// validate after compaction; validate region contents while redundancy recovery is running,
// wait for redundancy recovery and verify PR internals 
//ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_validateRegionContents;
//ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_waitForMyStartupRecovery
            //clientNames = fcn "hydra.TestConfigFcns.generateNames(\"dataStore\", ${dataStoreHosts}, true)" ncf;
//ENDTASK     taskClass   = parReg.ParRegTest  taskMethod = HydraTask_validateInternalPRState;

ENDTASK taskClass   = hdfs.HDFSUtil
        taskMethod  = stopCluster
        clientNames = locator1
        ;

THREADGROUP locator
  totalThreads = fcn  ${locatorHosts} * ${locatorVMsPerHost} * ${locatorThreadsPerVM} ncf
  totalVMs     = fcn "(${locatorHosts} * ${locatorVMsPerHost})" ncf
  clientNames  = fcn "hydra.TestConfigFcns.generateNames(\"locator\", ${locatorHosts}, true)" ncf;

// all threads in subgroup A are accessorThreads, and all threads in subgroup B
// are in dataStoreThreads
THREADGROUP accessorThreads
    totalThreads = fcn
                   ${${A}Hosts} * ${${A}VMsPerHost} * ${${A}ThreadsPerVM}
                   ncf
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"${A}\", ${${A}Hosts}, true)"
                   ncf;
THREADGROUP dataStoreThreads
    totalThreads = fcn
                   ${${B}Hosts} * ${${B}VMsPerHost} * ${${B}ThreadsPerVM}
                   ncf
    clientNames  = fcn "hydra.TestConfigFcns.generateNames
                        (\"${B}\", ${${B}Hosts}, true)"
                   ncf;

hydra.VmPrms-extraClassPaths +=
  fcn "hydra.HadoopPrms.getServerJars(\"$HADOOP_DIST\", ${locatorHosts})" ncf
  ,
  fcn "hydra.HadoopPrms.getServerJars(\"$HADOOP_DIST\", ${accessorHosts})" ncf
  ,
  fcn "hydra.HadoopPrms.getServerJars(\"$HADOOP_DIST\", ${dataStoreHosts})" ncf
  ;

// add another 100MB for the HDFS AEQ (maxMemory in MB) in the dataStores
hydra.VmPrms-extraVMArgs  += fcn "hydra.TestConfigFcns.duplicate
                                  (\"-Dnone\", ${${A}Hosts}, true)"
                             ncf
                             fcn "hydra.TestConfigFcns.duplicate
                                  (\"-Xms400m -Xmx400m \", ${${B}Hosts}, true)"
                             ncf;

hydra.GemFirePrms-stopSystemsAfterTest = true;

// this is the granularity for pausing
util.TestHelperPrms-minTaskGranularitySec = 10;
parReg.ParRegPrms-secondsToRun = 1800; // this is the time for the whole run
hydra.Prms-checkTaskMethodsExist = false;
hydra.Prms-alwaysDoEndTasks = true;
hydra.Prms-totalTaskTimeSec = 31536000;  // don't let hydra terminate based on time
hydra.Prms-maxResultWaitSec = 3600;  // allow more time for HDFS versions
hydra.Prms-serialExecution = false;
hydra.Prms-clientShutdownHook = parReg.ParRegUtil dumpAllPartitionedRegions;
hydra.Prms-useFixedRandomInMaster= true;

// End tasks for persistent peer tests; recover from disk and validate recovery 
hydra.Prms-doStartAndEndTasksLockStep = true;
hydra.Prms-maxEndTaskResultWaitSec = 7200;

INCLUDE $JTESTS/util/randomValues.inc;
util.RandomValuesPrms-objectType = byte[];
util.RandomValuesPrms-elementSize = 1000;
util.ValueHolderPrms-useExtraObject = true;
hydra.GemFirePrms-conserveSockets = ONEOF true false FOENO;

// don't do writes in servers that can be killed
parReg.ParRegPrms-designateOps = true;
parReg.ParRegPrms-bridgeOrderingWorkaround = uniqueKeys;
parReg.ParRegPrms-accessorOperations = ONEOF add putIfAbsentAsCreate add putIfAbsentAsCreate putAll destroy remove
                                             getNew getNew update get 
                                             replaceNoInval replaceOldNoInval FOENO;
parReg.ParRegPrms-dataStoreOperations = ONEOF get getNew FOENO; 
parReg.ParRegPrms-upperThreshold = 500;
parReg.ParRegPrms-upperThresholdAccessorOperations = ONEOF destroy remove FOENO;
parReg.ParRegPrms-upperThresholdDataStoreOperations = ONEOF get removeAsNoop replaceAsNoop replaceOldAsNoop FOENO;
parReg.ParRegPrms-lowerThreshold = 0;
parReg.ParRegPrms-lowerThresholdAccessorOperations = ONEOF add putIfAbsentAsCreate putAll FOENO;
parReg.ParRegPrms-lowerThresholdDataStoreOperations = ONEOF get update removeAsNoop replaceAsNoop replaceOldAsNoop FOENO;
            
parReg.ParRegPrms-entryOperations = notUsed; 
parReg.ParRegPrms-lowerThresholdOperations = notUsed;
parReg.ParRegPrms-upperThresholdOperations = notUsed;

parReg.ParRegPrms-numPutAllNewKeys = 1;
parReg.ParRegPrms-numPutAllExistingKeys = RANGE 1 100 EGNAR;

parReg.ParRegPrms-numVMsToStop = ${numVMsToStop};
parReg.ParRegPrms-localMaxMemory = RANGE 1 10 EGNAR;
util.StopStartPrms-stopModes = ONEOF NICE_EXIT MEAN_KILL MEAN_EXIT NICE_KILL FOENO;
parReg.ParRegPrms-highAvailability = true;


// for now, do not attempt to re-initialize regions based on generated cache.xml files
util.CachePrms-useDeclarativeXmlFile = false;

hydra.ConfigPrms-hadoopConfig = hadoop;
hydra.ConfigPrms-hdfsStoreConfig = hdfsstore;

hydra.CachePrms-names           = cache1;
hydra.CachePrms-searchTimeout   = 600;

hydra.RegionPrms-names          = clientRegion           dataStoreRegion;
hydra.RegionPrms-regionName     = partitionedRegion;
hydra.RegionPrms-dataPolicy     = hdfsPartition;
hydra.RegionPrms-hdfsStoreName  = hdfsstore;
hydra.RegionPrms-hdfsWriteOnly  = false;
hydra.RegionPrms-cacheLoader    = none; // don't cause a write with a loader
hydra.RegionPrms-cacheListeners = util.SummaryLogListener;
hydra.RegionPrms-partitionName  = accessorPR               dataStorePR;

hydra.PartitionPrms-names           = accessorPR          dataStorePR;
hydra.PartitionPrms-redundantCopies = ${redundantCopies};
hydra.PartitionPrms-localMaxMemory  = 0                   default;

hydra.HadoopPrms-names = hadoop;

hydra.HDFSStorePrms-names = hdfsstore;
hydra.HDFSStorePrms-hadoopName = hadoop;
hydra.HDFSStorePrms-diskStoreName = disk;
hydra.HDFSStorePrms-batchSizeMB = 5;
hydra.HDFSStorePrms-homeDir = gemfire_data;
hydra.HDFSStorePrms-maximumQueueMemory = 50;
hydra.HDFSStorePrms-persistent = true;

hydra.DiskStorePrms-names = disk;
hydra.DiskStorePrms-queueSize = ONEOF 1 5 10 20 FOENO;
hydra.DiskStorePrms-timeInterval = oneof 1 10 50 500 1000 2000 foeno;

// not used in this test
//hydra.HDFSStorePrms-batchTimeInterval = ????;
//hydra.HDFSStorePrms-blockCacheSize = ????;
//hydra.HDFSStorePrms-diskSynchronous = true/false;
//hydra.HDFSStorePrms-fileRolloverInterval = ????;
//hydra.HDFSStorePrms-maxFileSize = ????;
