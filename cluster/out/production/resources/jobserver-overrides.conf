# Settings for jobserver overriding the application.conf or bootProperties.
# ideally there should have been reference.conf only in the jobserver packaging.
#
# in generic case, user supplied application.conf will be picked up if found first in the classpath.
# in snappy embeded mode properties file will be parsed much earlier & converted into bootProperties.
# user won't need application.conf in snappydata's case.
#
# to provide snappy defaults "jobserver-defaults.conf" should be used as right now we aren't using typesafe.Config
#
# NOTE: Be careful in listing PROPERTIES here. These cannot be changed by the user unless mentioned in system properties.
spark {

  jobserver {

    # To load up job jars on startup, place them here,
    # with the app name as the key and the path to the jar as the value
    # job-jar-paths {
    #   test = ../job-server-tests/target/scala-2.10/job-server-tests_2.10-0.6.0.jar
    # }

    sqldao {
      ##[soubhik] we want to override all of these to point to SnappyData.

      # Slick database driver, full classpath
      slick-driver = scala.slick.driver.H2Driver

      # JDBC driver, full classpath
      jdbc-driver = org.h2.Driver

      # Directory where default H2 driver stores its data. Only needed for H2.
      rootdir = <basedir>/spark-jobserver/sqldao/data

      # Full JDBC URL / init string, along with username and password.  Sorry, needs to match above.
      # Substitutions may be used to launch job-server, but leave it out here in the default or tests won't pass
      jdbc {
        url = "jdbc:h2:file:/tmp/spark-jobserver/sqldao/data/h2-db"
        user = ""
        password = ""
      }

      ##[soubhik] for now allow user configuration.
      # DB connection pool settings
      #dbcp {
      #  maxactive = 20
      #  maxidle = 10
      #  initialsize = 10
      #}
    }
  }

  # Default settings for ad hoc as well as manually created contexts
  # You can add any Spark config params here, for example, spark.mesos.coarse = true
  context-settings {

    # A zero-arg class implementing spark.jobserver.context.SparkContextFactory
    # Determines the type of jobs that can run in a SparkContext
    context-factory = org.apache.spark.sql.SnappySessionFactory

    streaming {

      ##[soubhik] I think we want these to be non-overridable. right now leaving it upto user.

      # if true, stops gracefully by waiting for the processing of all received data to be completed
      #stopGracefully = true

      # if true, stops the SparkContext with the StreamingContext. The underlying SparkContext will be
      # stopped regardless of whether the StreamingContext has been started.
      #stopSparkContext = true
    }

  }

  contexts {

    snappyStreamingContext {
      # A zero-arg class implementing spark.jobserver.context.SparkContextFactory
      # Determines the type of jobs that can run in a SparkContext
      context-factory = org.apache.spark.sql.streaming.SnappyStreamingContextFactory

      streaming {
        # if true, stops gracefully by waiting for the processing of all received data to be completed
        stopGracefully = true

        # if true, stops the SparkContext with the StreamingContext. The underlying SparkContext will be
        # stopped regardless of whether the StreamingContext has been started.
        stopSparkContext = false
      }
    }
  }
}
