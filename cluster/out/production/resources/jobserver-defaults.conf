# Default settings of snappydata that will override application.conf in jobserver.
# If user provides application.conf in the classpath in the front and expects to override
# snappy defaults, it won't be honored as stated in ConfigFactory.load(), instead snappy
# users are expected to use snappydata.properties file to pass on defaults. Properties
# from that file will be converted into bootProperties which is placed on top of this
# and thus honored.

# NOTE: PROPERTIES present in "jobserver-overrides.conf" SHOULD NEVER EXIST HERE.


##[soubhik] right now I've just copied spark portion of the application.conf for
# our own defaults in case we don't agree with jobserver defaults.
spark {
  master = "local[4]"
  # spark web UI port
  webUrlPort = 8080

  jobserver {
    port = 8090
    bind-address = "0.0.0.0"

    # Number of job results to keep per JobResultActor/context
    job-result-cache-size = 5000

    jobdao = spark.jobserver.io.JobFileDAO

    # Automatically load a set of jars at startup time.  Key is the appName, value is the path/URL.
    # job-jar-paths {
    #   test = /path/to/my/test.jar
    # }

    filedao {
      rootdir = <basedir>/spark-jobserver/filedao/data
    }

    datadao {
      # storage directory for files that are uploaded to the server
      # via POST/data commands
      rootdir = <basedir>/spark-jobserver/upload
    }


    # To load up job jars on startup, place them here,
    # with the app name as the key and the path to the jar as the value
    # job-jar-paths {
    #   test = ../job-server-tests/target/scala-2.10/job-server-tests_2.10-0.6.0.jar
    # }

    sqldao {
      # DB connection pool settings
      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }

    # Time out for job server to wait while creating contexts
    context-creation-timeout = 15 s

    # Number of jobs that can be run simultaneously per context
    # If not set, defaults to number of cores on machine where jobserver is running
    max-jobs-per-context = 8

    # in yarn deployment, time out for job server to wait while creating contexts
    yarn-context-creation-timeout = 40 s

    # spark broadcst factory in yarn deployment
    # Versions prior to 1.1.0, spark default broadcast factory is org.apache.spark.broadcast.HttpBroadcastFactory.
    # Can't start multiple sparkContexts in the same JVM with HttpBroadcastFactory.
    yarn-broadcast-factory = org.apache.spark.broadcast.TorrentBroadcastFactory
  }

  # predefined Spark contexts
  # Below is an example, but do not uncomment it.   Everything defined here is carried over to
  # deploy-time configs, so they will be created in all environments.  :(
  contexts {
    # abc-demo {
    #   num-cpu-cores = 4            # Number of cores to allocate.  Required.
    #   memory-per-node = 1024m      # Executor memory per node, -Xmx style eg 512m, 1G, etc.
    # }
    # define additional contexts here
  }

  # Default settings for ad hoc as well as manually created contexts
  # You can add any Spark config params here, for example, spark.mesos.coarse = true
  context-settings {
    num-cpu-cores = 4           # Number of cores to allocate.  Required.
    memory-per-node = 512m      # Executor memory per node, -Xmx style eg 512m, 1G, etc.

    streaming {
      # Default batch interval for Spark Streaming contexts in milliseconds
      batch_interval = 1000

      # if true, stops gracefully by waiting for the processing of all received data to be completed
      stopGracefully = true

      # if true, stops the SparkContext with the StreamingContext. The underlying SparkContext will be
      # stopped regardless of whether the StreamingContext has been started.
      stopSparkContext = true
    }

    # uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]
    passthrough {
      ##[soubhik] changing the jobserver default from true to false. keeping it here, in case we will allow
      # Non-Embeded mode later.
      spark.driver.allowMultipleContexts = false  # Ignore the Multiple context exception related with SPARK-2243
    }
  }
}
