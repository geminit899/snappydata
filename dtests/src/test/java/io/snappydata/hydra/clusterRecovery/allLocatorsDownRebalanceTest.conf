hydra.Prms-testRequirement = "Test to verify cluster recovery when a new server node is added in the existing cluster.";
hydra.Prms-testDescription = "The test starts a snappy cluster ,populates data ,and records data count for validation.
Then all the locators are shut down and try adding a new server to the cluster. This fails as expected because no locators are up.
The cluster is restarted and try adding the new server again.This time the node should join the cluster.
Now simultaneously call  rebalance procedure and kill a server abruptly.
After rebalancing is completed, cluster is restarted and data validation is done. ";

INCLUDE $JTESTS/hydraconfig/hydraparams1.inc;
INCLUDE $JTESTS/hydraconfig/topology_1.inc;

//threadGroups
INCLUDE $JTESTS/io/snappydata/hydra/cdcConnector/threadGroups.inc;

//Initial threadGroups
INITTASK    taskClass   = io.snappydata.hydra.cluster.SnappyTest taskMethod  = initSnappyArtifacts
            runMode = always
            threadGroups = snappyThreads,snappyInitThread,snappyTaskThread1,snappyTaskThread2,snappyTaskThread3,snappyTaskThread4,snappyTaskThread5,snappyHAThread;

INITTASK    taskClass   = io.snappydata.hydra.cluster.SnappyTest taskMethod  = HydraTask_recordProcessIDWithHost
            runMode = always
            threadGroups = snappyThreads,snappyInitThread,snappyTaskThread1,snappyTaskThread2,snappyTaskThread3,snappyTaskThread4,snappyTaskThread5,snappyHAThread;

//Do Validation take data count of all the tables .
INITTASK    taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_storeDataCount
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-isBeforeRestart = true
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-dataLocation = ${queryPath}
            threadGroups = snappyInitThread;

//get locators down :
INITTASK    taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_performHA
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeName = ${locatorNode}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-snappyFileLoc = ${snappyPath}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeType = locators
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-isOnlyStop = true
            threadGroups = snappyInitThread;

//Add a new server node,when all locators are down (GemFireConfigException expected)
INITTASK    taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_addNewNode
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-dataLocation = ${logPath}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeName = ${newNode}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-snappyFileLoc = ${snappyPath}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeType = servers
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-isNewNodeFirst = false
            threadGroups = snappyInitThread;

//Restart the cluster,this will start the locators also
INITTASK    taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_clusterRestart
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-snappyFileLoc = ${snappyPath}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeType = allNodes
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-isStopStartCluster=false
            threadGroups = snappyInitThread;

//Again add the new server node,this time it should succeed.
INITTASK    taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_addNewNode
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-dataLocation = ${logPath}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeName = ${newNode}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-snappyFileLoc = ${snappyPath}
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeType = servers
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-isNewNodeFirst = false
            threadGroups = snappyInitThread;

// trigger rebalance
TASK       taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_performRebalance
           threadGroups = snappyTaskThread1
           maxTimesToRun = 1
           maxThreads = 1;

//While rebalance is on,issue mean kill.
TASK       taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_meanKillProcesses
           threadGroups = snappyTaskThread2
           io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeType = servers
           io.snappydata.hydra.cdcConnector.SnappyCDCPrms-numNodesToStop=1
           io.snappydata.hydra.cdcConnector.SnappyCDCPrms-snappyFileLoc = ${snappyPath}
           maxTimesToRun = 1
           maxThreads = 1;

//restart the cluster.
CLOSETASK  taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_clusterRestart
           io.snappydata.hydra.cdcConnector.SnappyCDCPrms-snappyFileLoc = ${snappyPath}
           io.snappydata.hydra.cdcConnector.SnappyCDCPrms-nodeType = allNodes
           io.snappydata.hydra.cdcConnector.SnappyCDCPrms-isStopStartCluster=true
           threadGroups = snappyInitThread;

CLOSETASK   taskClass  = io.snappydata.hydra.cdcConnector.SnappyCDCTest taskMethod  = HydraTask_validateDataCount
            io.snappydata.hydra.cdcConnector.SnappyCDCPrms-dataLocation = ${queryPath}
            threadGroups = snappyInitThread;

io.snappydata.hydra.cluster.SnappyPrms-userAppJar = snappydata-store-scala-tests*tests.jar;
io.snappydata.hydra.cluster.SnappyPrms-isLongRunningTest = true;
hydra.Prms-maxResultWaitSec = 10800; // 3 hours
hydra.Prms-totalTaskTimeSec = 18000; // 5 hours

